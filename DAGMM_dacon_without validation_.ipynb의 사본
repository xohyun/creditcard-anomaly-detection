{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DAGMM_dacon_without validation_.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPQLqmcgTdEOp9LL3F9SCA4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"d932eb7ff34e433db0fdc24d3f6f85c9":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_c59d56f11dfb4b2185180e8b5f9db004","IPY_MODEL_87cde288af34459ca54f3c590705f4fe"],"layout":"IPY_MODEL_37608db0892f408b895d525eb84fdf53"}},"c59d56f11dfb4b2185180e8b5f9db004":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b45cdc142a604259a0d41046f287444b","placeholder":"​","style":"IPY_MODEL_75d6bf262f87488d8d92f09f1d86a4bc","value":"0.015 MB of 0.015 MB uploaded (0.000 MB deduped)\r"}},"87cde288af34459ca54f3c590705f4fe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d9b8172799b409b9cffaaba7ae4b003","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abe2eea0bde24883a163de4ae09bef08","value":1}},"37608db0892f408b895d525eb84fdf53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b45cdc142a604259a0d41046f287444b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75d6bf262f87488d8d92f09f1d86a4bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d9b8172799b409b9cffaaba7ae4b003":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abe2eea0bde24883a163de4ae09bef08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ik9sLgGJ7G59","executionInfo":{"status":"ok","timestamp":1659084460684,"user_tz":-540,"elapsed":3677,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"0374e07e-aa3e-4a69-c575-d7187f9ffbf7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[Errno 2] No such file or directory: 'drive/MyDrive/IITP/sohyun/creditcard_prediction/data'\n","/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/MyDrive/IITP/sohyun/creditcard_prediction/data"]},{"cell_type":"code","source":["!pip install wandb -qqq\n","import wandb\n","wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P3CNrkhz2jjy","executionInfo":{"status":"ok","timestamp":1659084463485,"user_tz":-540,"elapsed":2809,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"2a2d234a-d130-4a0b-930f-7ebe488b3c59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":175}]},{"cell_type":"code","source":["torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","np.random.seed(0)\n","cudnn.benchmark = False\n","cudnn.deterministic = True\n","random.seed(0)"],"metadata":{"id":"61FVN9sCI2gt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","train_df = pd.read_csv('./train.csv')\n","train_df = train_df.drop(columns=['ID'])\n","val_df = pd.read_csv('./val.csv')\n","val_df = val_df.drop(columns=['ID'])\n","test_df = pd.read_csv('./test.csv')\n","test_df = test_df.drop(columns=['ID'])\n","\n","#-------------------#\n","#---# Normalize #---#\n","#-------------------#\n","# case 1 - standardscaler\n","scaler_n = StandardScaler()\n","scaler_n.fit(train_df)\n","\n","val_x = val_df.drop(columns=['Class'])\n","train_x_scaleN = pd.DataFrame(scaler_n.transform(train_df), columns = train_df.columns) # 확인 : train_x_scaleN.mean(), train_x_scaleN.var()\n","val_x_scaleN = pd.DataFrame(scaler_n.transform(val_x), columns = val_x.columns)\n","test_x_scaleN = pd.DataFrame(scaler_n.transform(test_df), columns = test_df.columns)\n","\n","train_df = train_x_scaleN\n","val_df = pd.concat([val_x_scaleN, pd.DataFrame(val_df['Class'])], axis=1)\n","test_df = test_x_scaleN"],"metadata":{"id":"hsvD9j7WUHEz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import numpy as np\n","\n","def to_var(x, volatile=False):\n","    if torch.cuda.is_available():\n","        x = x.cuda()\n","    return Variable(x, volatile=volatile)\n","\n","def mkdir(directory):\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)"],"metadata":{"id":"8ixkP9ze8mE2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import os\n","import random\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from PIL import Image\n","import h5py\n","import numpy as np\n","import collections\n","import numbers\n","import math\n","import pandas as pd\n","\n","class KDD99Loader(object):\n","    def __init__(self, data_path, mode=\"train\"):\n","        self.mode=mode\n","        # data = np.load(data_path)\n","\n","        # labels = data[\"kdd\"][:,-1]\n","        # features = data[\"kdd\"][:,:-1]\n","        # N, D = features.shape\n","        \n","        # normal_data = features[labels==1]\n","        # normal_labels = labels[labels==1]\n","\n","        # N_normal = normal_data.shape[0]\n","\n","        # attack_data = features[labels==0]\n","        # attack_labels = labels[labels==0]\n","\n","        # N_attack = attack_data.shape[0]\n","\n","        # randIdx = np.arange(N_attack)\n","        # np.random.shuffle(randIdx)\n","        # N_train = N_attack // 2\n","\n","        # self.train = attack_data[randIdx[:N_train]]\n","        # self.train_labels = attack_labels[randIdx[:N_train]]\n","\n","        # self.test = attack_data[randIdx[N_train:]]\n","        # self.test_labels = attack_labels[randIdx[N_train:]]\n","\n","        # self.test = np.concatenate((self.test, normal_data),axis=0)\n","        # self.test_labels = np.concatenate((self.test_labels, normal_labels),axis=0)\n","\n","        self.train = train_df.values\n","        self.train_labels = np.tile(0, len(train_df))\n","\n","        self.test = val_df.drop(columns = ['Class']).values\n","        self.test_labels = val_df['Class'].values\n","\n","        # self.test = test_df.values\n","        # self.test_labels = np.tile(0, len(test_df))\n","\n","    def __len__(self):\n","        \"\"\"\n","        Number of images in the object dataset.\n","        \"\"\"\n","        if self.mode == \"train\":\n","            return self.train.shape[0]\n","        else:\n","            return self.test.shape[0]\n","\n","    def __getitem__(self, index):\n","        if self.mode == \"train\":\n","            return np.float32(self.train[index]), np.float32(self.train_labels[index])\n","        else:\n","           return np.float32(self.test[index]), np.float32(self.test_labels[index])\n","        \n","\n","def get_loader(data_path, batch_size, mode='train'):\n","    \"\"\"Build and return data loader.\"\"\"\n","\n","    dataset = KDD99Loader(data_path, mode)\n","\n","    shuffle = False\n","    if mode == 'train':\n","        shuffle = True\n","\n","    data_loader = DataLoader(dataset=dataset,\n","                             batch_size=batch_size,\n","                             shuffle=shuffle)\n","    return data_loader"],"metadata":{"id":"uSLkzBK08gZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import torchvision\n","from torch.autograd import Variable\n","import itertools\n","\n","class Cholesky(torch.autograd.Function):\n","    def forward(ctx, a):\n","        l = torch.cholesky(a, False)\n","        ctx.save_for_backward(l)\n","        return l\n","    def backward(ctx, grad_output):\n","        l, = ctx.saved_variables\n","        linv = l.inverse()\n","        inner = torch.tril(torch.mm(l.t(), grad_output)) * torch.tril(\n","            1.0 - Variable(l.data.new(l.size(1)).fill_(0.5).diag()))\n","        s = torch.mm(linv.t(), torch.mm(inner, linv))\n","        return s\n","    \n","class DaGMM(nn.Module):\n","    \"\"\"Residual Block.\"\"\"\n","    def __init__(self, n_gmm = 4, latent_dim=12):\n","        super(DaGMM, self).__init__()\n","\n","        layers = []\n","        # layers += [nn.Linear(118,60)]\n","        # layers += [nn.Tanh()]        \n","        # layers += [nn.Linear(60,30)]\n","        # layers += [nn.Tanh()]        \n","        layers += [nn.Linear(30,25)]\n","        layers += [nn.Tanh()]         \n","        layers += [nn.Linear(25,20)]\n","        layers += [nn.Tanh()]         \n","        layers += [nn.Linear(20,10)]\n","        # layers += [nn.Tanh()]         \n","        # layers += [nn.Linear(15,10)]\n","\n","        self.encoder = nn.Sequential(*layers)\n","\n","        layers = []\n","        # layers += [nn.Linear(10,15)]\n","        # layers += [nn.Tanh()]        \n","        layers += [nn.Linear(10,20)]\n","        layers += [nn.Tanh()]        \n","        layers += [nn.Linear(20,25)]\n","        layers += [nn.Tanh()]        \n","        layers += [nn.Linear(25,30)]\n","        # layers += [nn.Tanh()]        \n","        # layers += [nn.Linear(60,118)]\n","\n","        self.decoder = nn.Sequential(*layers)\n","\n","        layers = []\n","        layers += [nn.Linear(latent_dim,10)]\n","        layers += [nn.Tanh()]        \n","        layers += [nn.Dropout(p=0.5)]        \n","        layers += [nn.Linear(10,n_gmm)]\n","        layers += [nn.Softmax(dim=1)]\n","\n","\n","        self.estimation = nn.Sequential(*layers)\n","\n","        self.register_buffer(\"phi\", torch.zeros(n_gmm))\n","        self.register_buffer(\"mu\", torch.zeros(n_gmm,latent_dim))\n","        self.register_buffer(\"cov\", torch.zeros(n_gmm,latent_dim,latent_dim))\n","\n","    def relative_euclidean_distance(self, a, b):\n","        return (a-b).norm(2, dim=1) / a.norm(2, dim=1)\n","\n","    def forward(self, x):\n","\n","        enc = self.encoder(x)\n","\n","        dec = self.decoder(enc)\n","\n","        rec_cosine = F.cosine_similarity(x, dec, dim=1)\n","        rec_euclidean = self.relative_euclidean_distance(x, dec)\n","\n","        z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n","\n","        gamma = self.estimation(z)\n","\n","        return enc, dec, z, gamma\n","\n","    def compute_gmm_params(self, z, gamma):\n","        N = gamma.size(0)\n","        # K\n","        sum_gamma = torch.sum(gamma, dim=0)\n","\n","        # K\n","        phi = (sum_gamma / N)\n","\n","        self.phi = phi.data\n","\n"," \n","        # K x D\n","        mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n","        self.mu = mu.data\n","        # z = N x D\n","        # mu = K x D\n","        # gamma N x K\n","\n","        # z_mu = N x K x D\n","        z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n","\n","        # z_mu_outer = N x K x D x D\n","        z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n","\n","        # K x D x D\n","        cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim = 0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n","        self.cov = cov.data\n","\n","        return phi, mu, cov\n","        \n","    def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n","        if phi is None:\n","            phi = to_var(self.phi)\n","        if mu is None:\n","            mu = to_var(self.mu)\n","        if cov is None:\n","            cov = to_var(self.cov)\n","\n","        k, D, _ = cov.size()\n","\n","        z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n","\n","        cov_inverse = []\n","        det_cov = []\n","        cov_diag = 0\n","        eps = 1e-12\n","        for i in range(k):\n","            # K x D x D\n","            cov_k = cov[i] + to_var(torch.eye(D)*eps)\n","            cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n","\n","            #det_cov.append(np.linalg.det(cov_k.data.cpu().numpy()* (2*np.pi)))\n","            det_cov.append((Cholesky.apply(cov_k.cpu() * (2*np.pi)).diag().prod()).unsqueeze(0))\n","            cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n","\n","        # K x D x D\n","        cov_inverse = torch.cat(cov_inverse, dim=0)\n","        # K\n","        det_cov = torch.cat(det_cov).cuda()\n","        #det_cov = to_var(torch.from_numpy(np.float32(np.array(det_cov))))\n","\n","        # N x K\n","        exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n","        # for stability (logsumexp)\n","        max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n","\n","        exp_term = torch.exp(exp_term_tmp - max_val)\n","\n","        # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (det_cov).unsqueeze(0), dim = 1) + eps)\n","        sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt(det_cov)).unsqueeze(0), dim = 1) + eps)\n","        # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt((2*np.pi)**D * det_cov)).unsqueeze(0), dim = 1) + eps)\n","\n","\n","        if size_average:\n","            sample_energy = torch.mean(sample_energy)\n","\n","        return sample_energy, cov_diag\n","\n","\n","    def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n","\n","        recon_error = torch.mean((x - x_hat) ** 2)\n","\n","        phi, mu, cov = self.compute_gmm_params(z, gamma)\n","\n","        sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n","\n","        loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n","\n","        return loss, sample_energy, recon_error, cov_diag"],"metadata":{"id":"MplRhuqf8CBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import os\n","import time\n","import datetime\n","from torch.autograd import grad\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","import IPython\n","from tqdm import tqdm\n","\n","class Solver(object):\n","    DEFAULTS = {}   \n","    def __init__(self, data_loader, config):\n","        # Data loader\n","        self.__dict__.update(Solver.DEFAULTS, **config)\n","        self.data_loader = data_loader\n","\n","        # Build tensorboard if use\n","        self.build_model()\n","        # if self.use_tensorboard:\n","        #     self.build_tensorboard()\n","\n","        # Start with trained model\n","        if self.pretrained_model:\n","            self.load_pretrained_model()\n","\n","    def build_model(self):\n","        # Define model\n","        self.dagmm = DaGMM(self.gmm_k)\n","\n","        # Optimizers\n","        self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr)\n","\n","        # Print networks\n","        self.print_network(self.dagmm, 'DaGMM')\n","\n","        if torch.cuda.is_available():\n","            self.dagmm.cuda()\n","\n","    def print_network(self, model, name):\n","        num_params = 0\n","        for p in model.parameters():\n","            num_params += p.numel()\n","        print(name)\n","        print(model)\n","        print(\"The number of parameters: {}\".format(num_params))\n","\n","    def load_pretrained_model(self):\n","        self.dagmm.load_state_dict(torch.load(os.path.join(\n","            self.model_save_path, '{}_dagmm.pth'.format(self.pretrained_model))))\n","\n","        # print(\"phi\", self.dagmm.phi,\"mu\",self.dagmm.mu, \"cov\",self.dagmm.cov)\n","\n","        print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n","\n","    # def build_tensorboard(self):\n","    #     from logger import Logger\n","    #     self.logger = Logger(self.log_path)\n","\n","    def reset_grad(self):\n","        self.dagmm.zero_grad()\n","\n","    def to_var(self, x, volatile=False):\n","        if torch.cuda.is_available():\n","            x = x.cuda()\n","        return Variable(x, volatile=volatile)\n","\n","    def train(self):\n","        iters_per_epoch = len(self.data_loader)\n","\n","        # Start with trained model if exists\n","        if self.pretrained_model:\n","            start = int(self.pretrained_model.split('_')[0])\n","        else:\n","            start = 0\n","\n","        # Start training\n","        iter_ctr = 0\n","        start_time = time.time()\n","      \n","        self.ap_global_train = np.array([0,0,0])\n","        for e in range(start, self.num_epochs):\n","            sum_total_loss = 0; sum_sample_energy = 0; sum_recon_error = 0; sum_cov_diag = 0 \n","            for i, (input_data, labels) in enumerate(tqdm(self.data_loader)):\n","                iter_ctr += 1\n","                start = time.time()\n","\n","                input_data = self.to_var(input_data)\n","\n","                total_loss,sample_energy, recon_error, cov_diag = self.dagmm_step(input_data)\n","                # Logging\n","                loss = {}\n","                loss['total_loss'] = total_loss.data.item()\n","                loss['sample_energy'] = sample_energy.item()\n","                loss['recon_error'] = recon_error.item()\n","                loss['cov_diag'] = cov_diag.item()\n","\n","                sum_total_loss += total_loss.data.item()\n","                sum_sample_energy += sample_energy.item()\n","                sum_recon_error += recon_error.item()\n","                sum_cov_diag += cov_diag.item()\n","\n","\n","                # Print out log info\n","                # if (i+1) % self.log_step == 0:\n","                #     elapsed = time.time() - start_time\n","                #     total_time = ((self.num_epochs*iters_per_epoch)-(e*iters_per_epoch+i)) * elapsed/(e*iters_per_epoch+i+1)\n","                #     epoch_time = (iters_per_epoch-i)* elapsed/(e*iters_per_epoch+i+1)\n","                    \n","                #     epoch_time = str(datetime.timedelta(seconds=epoch_time))\n","                #     total_time = str(datetime.timedelta(seconds=total_time))\n","                #     elapsed = str(datetime.timedelta(seconds=elapsed))\n","\n","                #     lr_tmp = []\n","                #     for param_group in self.optimizer.param_groups:\n","                #         lr_tmp.append(param_group['lr'])\n","                #     tmplr = np.squeeze(np.array(lr_tmp))\n","\n","                #     log = \"Elapsed {}/{} -- {} , Epoch [{}/{}], Iter [{}/{}], lr {}\".format(\n","                #         elapsed,epoch_time,total_time, e+1, self.num_epochs, i+1, iters_per_epoch, tmplr)\n","\n","                #     for tag, value in loss.items():\n","                #         log += \", {}: {:.4f}\".format(tag, value)\n","\n","                #     IPython.display.clear_output()\n","                #     print(log)\n","\n","                    # if self.use_tensorboard:\n","                    #     for tag, value in loss.items():\n","                    #         self.logger.scalar_summary(tag, value, e * iters_per_epoch + i + 1)\n","                    # else:\n","                    #     plt_ctr = 1\n","                    #     if not hasattr(self,\"loss_logs\"):\n","                    #         self.loss_logs = {}\n","                    #         for loss_key in loss:\n","                    #             self.loss_logs[loss_key] = [loss[loss_key]]\n","                    #             plt.subplot(2,2,plt_ctr)\n","                    #             plt.plot(np.array(self.loss_logs[loss_key]), label=loss_key)\n","                    #             plt.legend()\n","                    #             plt_ctr += 1\n","                    #     else:\n","                    #         for loss_key in loss:\n","                    #             self.loss_logs[loss_key].append(loss[loss_key])\n","                    #             plt.subplot(2,2,plt_ctr)\n","                    #             plt.plot(np.array(self.loss_logs[loss_key]), label=loss_key)\n","                    #             plt.legend()\n","                    #             plt_ctr += 1\n","\n","                    #     plt.show()\n","\n","                # print(\"phi\", self.dagmm.phi,\"mu\",self.dagmm.mu, \"cov\",self.dagmm.cov)\n","                # Save model checkpoints\n","            \n","                if (i+1) % self.model_save_step == 0:\n","                    torch.save(self.dagmm.state_dict(),\n","                        os.path.join(self.model_save_path, '{}_{}_dagmm.pth'.format(e+1, i+1)))\n","                    \n","            print(\"total_e\", sum_total_loss)\n","            print(\"\", sum_sample_energy)\n","            print(\"re\", sum_recon_error)\n","            print(\"cov\", sum_cov_diag)\n","            print(\"--\")\n","            wandb.log({\n","                \"total loss\": sum_total_loss,\n","                \"sample energy\": sum_sample_energy,\n","                \"recon error\": sum_recon_error,\n","                \"cov\": sum_cov_diag\n","            })\n","\n","    def dagmm_step(self, input_data):\n","        self.dagmm.train()\n","        enc, dec, z, gamma = self.dagmm(input_data)\n","\n","        total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma, self.lambda_energy, self.lambda_cov_diag)\n","\n","        self.reset_grad()\n","        total_loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(self.dagmm.parameters(), 5)\n","        self.optimizer.step()\n","\n","        return total_loss,sample_energy, recon_error, cov_diag\n","\n","    def test(self):\n","        print(\"======================TEST MODE======================\")\n","        self.dagmm.eval()\n","        self.data_loader.dataset.mode=\"train\"\n","\n","        N = 0\n","        mu_sum = 0\n","        cov_sum = 0\n","        gamma_sum = 0\n","\n","        for it, (input_data, labels) in enumerate(self.data_loader):\n","            input_data = self.to_var(input_data)\n","            enc, dec, z, gamma = self.dagmm(input_data)\n","            phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n","            \n","            batch_gamma_sum = torch.sum(gamma, dim=0)\n","            \n","            gamma_sum += batch_gamma_sum\n","            mu_sum += mu * batch_gamma_sum.unsqueeze(-1) # keep sums of the numerator only\n","            cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1) # keep sums of the numerator only\n","            \n","            N += input_data.size(0)\n","            \n","        train_phi = gamma_sum / N\n","        train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n","        train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n","\n","        # print(\"N:\",N)\n","        # print(\"phi :\\n\",train_phi)\n","        # print(\"mu :\\n\",train_mu)\n","        # print(\"cov :\\n\",train_cov)\n","\n","        train_energy = []\n","        train_labels = []\n","        train_z = []\n","        for it, (input_data, labels) in enumerate(self.data_loader):\n","            input_data = self.to_var(input_data)\n","            enc, dec, z, gamma = self.dagmm(input_data)\n","            sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","            \n","            train_energy.append(sample_energy.data.cpu().numpy())\n","            train_z.append(z.data.cpu().numpy())\n","            train_labels.append(labels.numpy())\n","\n","\n","        train_energy = np.concatenate(train_energy,axis=0)\n","        train_z = np.concatenate(train_z,axis=0)\n","        train_labels = np.concatenate(train_labels,axis=0)\n","\n","\n","        self.data_loader.dataset.mode=\"test\"\n","        test_energy = []\n","        test_labels = []\n","        test_z = []\n","        for it, (input_data, labels) in enumerate(self.data_loader):\n","            input_data = self.to_var(input_data)\n","            enc, dec, z, gamma = self.dagmm(input_data)\n","            sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","            test_energy.append(sample_energy.data.cpu().numpy())\n","            test_z.append(z.data.cpu().numpy())\n","            test_labels.append(labels.numpy())\n","\n","        test_energy = np.concatenate(test_energy,axis=0)\n","        test_z = np.concatenate(test_z,axis=0)\n","        test_labels = np.concatenate(test_labels,axis=0)\n","\n","        combined_energy = np.concatenate([train_energy, test_energy], axis=0)\n","        combined_labels = np.concatenate([train_labels, test_labels], axis=0)\n","\n","        thresh = np.percentile(combined_energy, 100 - 0.1)\n","        print(\"Threshold :\", thresh)\n","\n","        pred = (test_energy > thresh).astype(int)\n","        gt = test_labels.astype(int)\n","        print(\"================================sum\", sum(gt))\n","        from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n","\n","        accuracy = accuracy_score(gt,pred)\n","        precision, recall, f_score, support = prf(gt, pred, average='macro')\n","        \n","        print(\"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(accuracy, precision, recall, f_score))\n","        \n","        return accuracy, precision, recall, f_score"],"metadata":{"id":"0hkmHEtj8Pce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import argparse\n","from torch.backends import cudnn\n","\n","def str2bool(v):\n","    return v.lower() in ('true')\n","\n","def main(config):\n","    # For fast training\n","    cudnn.benchmark = True\n","\n","    # Create directories if not exist\n","    mkdir(config.log_path)\n","    mkdir(config.model_save_path)\n","\n","    data_loader = get_loader(config.data_path, batch_size=config.batch_size, mode=config.mode)\n","    \n","    # Solver\n","    solver = Solver(data_loader, vars(config))\n","\n","    if config.mode == 'train':\n","        solver.train()\n","    elif config.mode == 'test':\n","        solver.test()\n","\n","    return solver\n","    \n","if __name__ == '__main__':\n","    # parser = argparse.ArgumentParser()\n","\n","    # # Model hyper-parameters\n","    # parser.add_argument('--lr', type=float, default=1e-4)\n","\n","\n","    # # Training settings\n","    # parser.add_argument('--num_epochs', type=int, default=200)\n","    # parser.add_argument('--batch_size', type=int, default=1024)\n","    # parser.add_argument('--gmm_k', type=int, default=4)\n","    # parser.add_argument('--lambda_energy', type=float, default=0.1)\n","    # parser.add_argument('--lambda_cov_diag', type=float, default=0.005)\n","    # parser.add_argument('--pretrained_model', type=str, default=None)\n","\n","    # # Misc\n","    # parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n","    # parser.add_argument('--use_tensorboard', type=str2bool, default=None)\n","\n","    # # Path\n","    # parser.add_argument('--data_path', type=str, default='kdd_cup.npz')\n","    # parser.add_argument('--log_path', type=str, default='./dagmm/logs')\n","    # parser.add_argument('--model_save_path', type=str, default='./dagmm/models')\n","\n","    # # Step size\n","    # parser.add_argument('--log_step', type=int, default=10)\n","    # parser.add_argument('--sample_step', type=int, default=194)\n","    # parser.add_argument('--model_save_step', type=int, default=194)\n","\n","    # config = parser.parse_args()\n","    # args = vars(config)\n","    wandb.init()\n","    import easydict\n","    args = easydict.EasyDict({\n","        \"num_epochs\" : 40,\n","        \"batch_size\" : 4096,\n","        \"gmm_k\" : 3,\n","        \"lambda_energy\" : 0.1,\n","        \"lambda_cov_diag\" : 0.005,\n","        # \"pretrained_model\" : '',\n","        \"pretrained_model\" : None,\n","        \"mode\" : 'train',\n","        # \"mode\" : \"test\",\n","        \"data_path\" : \"./kdd_cup.npz\",   \n","        \"use_tensorboard\" : False,\n","        \"log_path\" : './logs',\n","        \"model_save_path\" : './models',\n","        \"log_step\" : 194//4,\n","        \"sample_step\" : 194,\n","        \"model_save_step\" : 194,\n","        \"lr\" : 1e-7, #2e-5\n","        \"wd\" : None\n","    })\n","    config = args\n"," \n","    print('------------ Options -------------')\n","    for k, v in sorted(args.items()):\n","        print('%s: %s' % (str(k), str(v)))\n","    print('-------------- End ----------------')\n","\n","    solver = main(config)\n","    solver.test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d932eb7ff34e433db0fdc24d3f6f85c9","c59d56f11dfb4b2185180e8b5f9db004","87cde288af34459ca54f3c590705f4fe","37608db0892f408b895d525eb84fdf53","b45cdc142a604259a0d41046f287444b","75d6bf262f87488d8d92f09f1d86a4bc","5d9b8172799b409b9cffaaba7ae4b003","abe2eea0bde24883a163de4ae09bef08"]},"id":"O0vLb9Gp7pjL","executionInfo":{"status":"ok","timestamp":1659085287673,"user_tz":-540,"elapsed":53411,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"e45eabdd-9cb8-4734-947a-379bd04567bd"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:2jfqyvr0) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d932eb7ff34e433db0fdc24d3f6f85c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cov</td><td>██▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>recon error</td><td>▃▂▇▃▄▅▆█▂▂▄▃▄▅█▁▅▄▃▄▅▁▅▃▂█▃▅▆▄</td></tr><tr><td>sample energy</td><td>▁▁▁▂▂▂▂▂▂▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>total loss</td><td>██▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cov</td><td>114406.05078</td></tr><tr><td>recon error</td><td>28.76752</td></tr><tr><td>sample energy</td><td>-113.79489</td></tr><tr><td>total loss</td><td>589.41826</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">summer-lion-216</strong>: <a href=\"https://wandb.ai/sohyun/uncategorized/runs/2jfqyvr0\" target=\"_blank\">https://wandb.ai/sohyun/uncategorized/runs/2jfqyvr0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20220729_085940-2jfqyvr0/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:2jfqyvr0). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.12.21"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data/wandb/run-20220729_090034-2g9itmxg</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/sohyun/uncategorized/runs/2g9itmxg\" target=\"_blank\">wise-totem-217</a></strong> to <a href=\"https://wandb.ai/sohyun/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["------------ Options -------------\n","batch_size: 4096\n","data_path: ./kdd_cup.npz\n","gmm_k: 3\n","lambda_cov_diag: 0.005\n","lambda_energy: 0.1\n","log_path: ./logs\n","log_step: 48\n","lr: 1e-07\n","mode: train\n","model_save_path: ./models\n","model_save_step: 194\n","num_epochs: 40\n","pretrained_model: None\n","sample_step: 194\n","use_tensorboard: False\n","wd: None\n","-------------- End ----------------\n","DaGMM\n","DaGMM(\n","  (encoder): Sequential(\n","    (0): Linear(in_features=30, out_features=25, bias=True)\n","    (1): Tanh()\n","    (2): Linear(in_features=25, out_features=20, bias=True)\n","    (3): Tanh()\n","    (4): Linear(in_features=20, out_features=10, bias=True)\n","  )\n","  (decoder): Sequential(\n","    (0): Linear(in_features=10, out_features=20, bias=True)\n","    (1): Tanh()\n","    (2): Linear(in_features=20, out_features=25, bias=True)\n","    (3): Tanh()\n","    (4): Linear(in_features=25, out_features=30, bias=True)\n","  )\n","  (estimation): Sequential(\n","    (0): Linear(in_features=12, out_features=10, bias=True)\n","    (1): Tanh()\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=10, out_features=3, bias=True)\n","    (4): Softmax(dim=1)\n","  )\n",")\n","The number of parameters: 3193\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/28 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n","  from ipykernel import kernelapp as app\n","100%|██████████| 28/28 [00:01<00:00, 22.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 869.4677467346191\n"," -117.32199668884277\n","re 28.40198701620102\n","cov 170559.595703125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 26.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 869.4737644195557\n"," -117.29233980178833\n","re 28.39971625804901\n","cov 170560.6611328125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 22.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 869.0737895965576\n"," -117.24512004852295\n","re 28.41415637731552\n","cov 170476.8330078125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 26.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 868.5020084381104\n"," -117.24782466888428\n","re 28.400967061519623\n","cov 170365.16748046875\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 23.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 868.5261516571045\n"," -117.24624872207642\n","re 28.4195916056633\n","cov 170366.2412109375\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 26.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 868.3472442626953\n"," -117.26732778549194\n","re 28.410320162773132\n","cov 170332.7353515625\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 23.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 867.7642402648926\n"," -117.23644542694092\n","re 28.425800502300262\n","cov 170212.42041015625\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 27.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 867.6755886077881\n"," -117.26252269744873\n","re 28.406681895256042\n","cov 170199.0361328125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 26.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 867.4782028198242\n"," -117.24297571182251\n","re 28.395133435726166\n","cov 170161.4765625\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 22.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 866.8347911834717\n"," -117.20593547821045\n","re 28.401256382465363\n","cov 170030.82958984375\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 27.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 866.9168491363525\n"," -117.19994497299194\n","re 28.416288554668427\n","cov 170044.11474609375\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 23.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 866.6154117584229\n"," -117.17615795135498\n","re 28.43572324514389\n","cov 169979.4658203125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 27.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 866.3116817474365\n"," -117.19954061508179\n","re 28.41274404525757\n","cov 169923.78173828125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 23.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 866.0266227722168\n"," -117.15777921676636\n","re 28.418179750442505\n","cov 169864.84765625\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:00<00:00, 28.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 865.7313842773438\n"," -117.15106344223022\n","re 28.395094633102417\n","cov 169810.28271484375\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 24.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 865.25852394104\n"," -117.1501317024231\n","re 28.407211482524872\n","cov 169713.267578125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 27.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 865.3456287384033\n"," -117.11731433868408\n","re 28.41825270652771\n","cov 169727.826171875\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 23.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 864.8171691894531\n"," -117.11792469024658\n","re 28.40563666820526\n","cov 169624.669921875\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:00<00:00, 28.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 864.5735988616943\n"," -117.13120079040527\n","re 28.418329656124115\n","cov 169573.681640625\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 23.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 864.1655349731445\n"," -117.08287954330444\n","re 28.415587246418\n","cov 169491.65087890625\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 28.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 864.0149307250977\n"," -117.09433889389038\n","re 28.388965249061584\n","cov 169467.0849609375\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 27.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 863.4869728088379\n"," -117.08147525787354\n","re 28.409188389778137\n","cov 169357.189453125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 23.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 863.4424934387207\n"," -117.05686092376709\n","re 28.406045496463776\n","cov 169348.43017578125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 25.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 863.0201816558838\n"," -117.07484817504883\n","re 28.410436689853668\n","cov 169263.44970703125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 22.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 863.0118598937988\n"," -117.04350137710571\n","re 28.400683760643005\n","cov 169263.1083984375\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 25.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 862.3270816802979\n"," -117.0244688987732\n","re 28.408025920391083\n","cov 169124.30322265625\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 23.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 862.2835063934326\n"," -117.02774477005005\n","re 28.394751965999603\n","cov 169118.30859375\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 27.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 862.1441059112549\n"," -116.99699544906616\n","re 28.393895983695984\n","cov 169089.98681640625\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 24.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 861.6532154083252\n"," -116.97739839553833\n","re 28.40357506275177\n","cov 168989.4814453125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 26.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 861.4800834655762\n"," -116.98070287704468\n","re 28.400834381580353\n","cov 168955.4677734375\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 23.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 861.1434078216553\n"," -116.97408676147461\n","re 28.407648861408234\n","cov 168886.63671875\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 26.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 860.9634895324707\n"," -116.97617673873901\n","re 28.402265310287476\n","cov 168851.77197265625\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 21.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 861.0218658447266\n"," -116.95290470123291\n","re 28.4299938082695\n","cov 168857.43603515625\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 27.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 860.4270687103271\n"," -116.96421480178833\n","re 28.406883120536804\n","cov 168743.326171875\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 21.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 860.232837677002\n"," -116.93610334396362\n","re 28.409966349601746\n","cov 168703.30078125\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 26.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 859.651575088501\n"," -116.91288089752197\n","re 28.412887692451477\n","cov 168585.998046875\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 25.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 859.5544033050537\n"," -116.89129877090454\n","re 28.4502894282341\n","cov 168558.654296875\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 21.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 858.8670101165771\n"," -116.8818244934082\n","re 28.414733052253723\n","cov 168428.0966796875\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 25.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 858.8058128356934\n"," -116.87693071365356\n","re 28.428741931915283\n","cov 168412.95654296875\n","--\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 23.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["total_e 858.5466232299805\n"," -116.90988492965698\n","re 28.383657574653625\n","cov 168370.7939453125\n","--\n","======================TEST MODE======================\n","Threshold : 27.63097647476196\n","================================sum 30\n","Accuracy : 0.9992, Precision : 0.8122, Recall : 0.7498, F-score : 0.7776\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"AFJO8kET6dTU"},"execution_count":null,"outputs":[]}]}