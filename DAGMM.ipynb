{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DAGMM.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"11l-EZ3jh4m35otW_xCiLfxOGbFPrhGwe","authorship_tag":"ABX9TyOOwbMjdj8UDdbdjChndqcM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Sf4Jnkjo2Lo","executionInfo":{"status":"ok","timestamp":1658908909427,"user_tz":-540,"elapsed":2530,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"ec27bd6b-627f-4f49-f9e4-2ec98fa36015"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n"]}],"source":["from google.colab import drive\n","# drive.mount('/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data')\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/IITP/sohyun/creditcard_prediction/data"]},{"cell_type":"code","source":["!pip install wandb -qqq\n","import wandb\n","wandb.login()"],"metadata":{"id":"-hVhT5DjqRDT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658908916736,"user_tz":-540,"elapsed":4647,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"f1a43355-bdf6-4d7d-8fa6-938025eb71b4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msohyun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import os\n","import time\n","import datetime\n","from torch.autograd import grad\n","from torch.autograd import Variable\n","# from model import *\n","import matplotlib.pyplot as plt\n","# from utils import *\n","# from data_loader import *\n","import IPython\n","from tqdm import tqdm\n","from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n","import random\n","import torch.backends.cudnn as cudnn\n","from torch.utils.data import Dataset, DataLoader\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","np.random.seed(0)\n","cudnn.benchmark = False\n","cudnn.deterministic = True\n","random.seed(0)\n","\n","class Solver(object):\n","  DEFAULTS = {}   \n","  def __init__(self, device, data_loader, data_loader_v, data_loader_test, config):\n","    # Data loader\n","    self.__dict__.update(Solver.DEFAULTS, **config)\n","\n","    self.data_loader = data_loader\n","    self.data_loader_v = data_loader_v\n","    self.data_loader_test = data_loader_test\n","    self.device = device\n","\n","    # Build tensorboard if use\n","    self.build_model()\n","    if self.use_tensorboard:\n","      self.build_tensorboard()\n","\n","    # Start with trained model\n","    if self.pretrained_model:\n","      self.load_pretrained_model()\n","\n","  def build_model(self):\n","    # Define model\n","    self.dagmm = DaGMM(self.device, self.gmm_k)\n","\n","    # Optimizers\n","    if self.wd :\n","      self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr, weight_decay=self.wd) #weight_decay=0.1\n","    else :\n","      self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr) #weight_decay=0.1\n","\n","    # Print networks\n","    self.print_network(self.dagmm, 'DaGMM')\n","\n","    if torch.cuda.is_available():\n","      self.dagmm.cuda()\n","\n","  def print_network(self, model, name):\n","    num_params = 0\n","    for p in model.parameters():\n","      num_params += p.numel()\n","    print(name)\n","    # print(model)\n","    print(\"The number of parameters: {}\".format(num_params))\n","\n","  def load_pretrained_model(self):\n","    self.dagmm.load_state_dict(torch.load(os.path.join(\n","      self.model_save_path, '{}_dagmm.pth'.format(self.pretrained_model))))\n","\n","    print(\"phi\", self.dagmm.phi,\"mu\",self.dagmm.mu, \"cov\", self.dagmm.cov)\n","    print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n","\n","  # def build_tensorboard(self):\n","  #     from logger import Logger\n","  #     self.logger = Logger(self.log_path)\n","\n","  def reset_grad(self):\n","    self.dagmm.zero_grad()\n","\n","  def to_var(self, x, volatile=False):\n","    if torch.cuda.is_available():\n","      x = x.cuda()\n","    return Variable(x, volatile=volatile)\n","\n","  def train(self):\n","    iters_per_epoch = len(self.data_loader)\n","\n","    # Start with trained model if exists\n","    if self.pretrained_model : start = int(self.pretrained_model.split('_')[0])\n","    else : start = 0\n","\n","    # Start training\n","    iter_ctr = 0\n","    start_time = time.time()\n","    best_f_score = -1\n","    self.ap_global_train = np.array([0,0,0])\n","    for e in range(start, self.num_epochs):\n","      for i, (input_data, labels) in enumerate(tqdm(self.data_loader)):\n","        iter_ctr += 1\n","        start = time.time()\n","\n","        input_data = self.to_var(input_data)\n","\n","        total_loss,sample_energy, recon_error, cov_diag = self.dagmm_step(input_data)\n","        \n","        # Logging\n","        loss = {}\n","        loss['total_loss'] = total_loss.data.item()\n","        loss['sample_energy'] = sample_energy.item()\n","        loss['recon_error'] = recon_error.item()\n","        loss['cov_diag'] = cov_diag.item()\n","\n","        # Print out log info\n","        if (i+1) % self.log_step == 0:\n","          elapsed = time.time() - start_time\n","          total_time = ((self.num_epochs*iters_per_epoch)-(e*iters_per_epoch+i)) * elapsed/(e*iters_per_epoch+i+1)\n","          epoch_time = (iters_per_epoch-i)* elapsed/(e*iters_per_epoch+i+1)\n","          \n","          epoch_time = str(datetime.timedelta(seconds=epoch_time))\n","          total_time = str(datetime.timedelta(seconds=total_time))\n","          elapsed = str(datetime.timedelta(seconds=elapsed))\n","\n","          lr_tmp = []\n","          for param_group in self.optimizer.param_groups:\n","            lr_tmp.append(param_group['lr'])\n","          tmplr = np.squeeze(np.array(lr_tmp))\n","\n","          log = \"Elapsed {}/{} -- {} , Epoch [{}/{}], Iter [{}/{}], lr {}\".format(\n","            elapsed,epoch_time,total_time, e+1, self.num_epochs, i+1, iters_per_epoch, tmplr)\n","\n","          for tag, value in loss.items():\n","            log += \", {}: {:.4f}\".format(tag, value)\n","\n","          # Save model checkpoints\n","          # if (i+1) % self.model_save_step == 0:\n","          #   torch.save(self.dagmm.state_dict(),\n","          #     os.path.join(self.model_save_path, '{}_{}_dagmm.pth'.format(e+1, i+1)))\n","      \n","      # epoch 마다\n","      accuracy, precision, recall, f_score = self.validation()\n","      wandb.log({\"accuracy\":accuracy,\n","              \"precision\":precision,\n","              \"recall\":recall,\n","              \"f_score\":f_score,\n","              \"sample_energy\":sample_energy.item(),\n","              \"recpm_error\":recon_error.item(),\n","              \"cov_diag\":cov_diag.item(),\n","              \"total_loss\":total_loss.data.item()})\n","      if f_score > best_f_score : \n","        best_f_score = f_score\n","        torch.save(self.dagmm.state_dict(), os.path.join(self.model_save_path, '{}_{}_dagmm.pth'.format(e+1, i+1))) # Save model checkpoints\n","                \n","  def dagmm_step(self, input_data):\n","    self.dagmm.train()\n","    enc, dec, z, gamma = self.dagmm(input_data)\n","    total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma, self.lambda_energy, self.lambda_cov_diag)\n","\n","    self.reset_grad()\n","    total_loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(self.dagmm.parameters(), 5)\n","    self.optimizer.step()\n","\n","    return total_loss,sample_energy, recon_error, cov_diag\n","\n","  def validation(self):\n","    self.dagmm.eval()\n","    N = 0; mu_sum = 0; cov_sum = 0; gamma_sum = 0\n","\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n","      \n","      batch_gamma_sum = torch.sum(gamma, dim=0)\n","      \n","      gamma_sum += batch_gamma_sum\n","      mu_sum += mu * batch_gamma_sum.unsqueeze(-1) # keep sums of the numerator only\n","      cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1) # keep sums of the numerator only\n","      \n","      N += input_data.size(0)\n","        \n","    train_phi = gamma_sum / N\n","    train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n","    train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n","\n","    train_energy = []; train_labels = []; train_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      \n","      train_energy.append(sample_energy.data.cpu().numpy())\n","      train_z.append(z.data.cpu().numpy())\n","      train_labels.append(labels.numpy())\n","\n","    train_energy = np.concatenate(train_energy,axis=0)\n","    train_z = np.concatenate(train_z,axis=0)\n","    train_labels = np.concatenate(train_labels,axis=0)\n","\n","    # use validation data\n","    valid_energy = []; valid_labels = []; valid_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader_v):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, size_average=False)\n","      valid_energy.append(sample_energy.data.cpu().numpy())\n","      valid_z.append(z.data.cpu().numpy())\n","      valid_labels.append(labels.numpy())\n","\n","    valid_energy = np.concatenate(valid_energy,axis=0)\n","    valid_z = np.concatenate(valid_z,axis=0)\n","    valid_labels = np.concatenate(valid_labels,axis=0)\n","\n","    combined_energy = np.concatenate([train_energy, valid_energy], axis=0)\n","    combined_labels = np.concatenate([train_labels, valid_labels], axis=0)\n","\n","    thresh = np.percentile(combined_energy, 100 - 0.3)\n","    # print(\"Threshold :\", thresh)\n","\n","    pred = (valid_energy > thresh).astype(int)\n","    gt = valid_labels.astype(int)\n","\n","    accuracy = accuracy_score(gt,pred)\n","    precision, recall, f_score, support = prf(gt, pred, average='macro', labels=[0,1])\n","    print(\"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(accuracy, precision, recall, f_score))\n","    return accuracy, precision, recall, f_score\n","\n","  def test(self):\n","    print(\"======================TEST MODE======================\")\n","    self.dagmm.eval()\n","    ### use train data\n","    N = 0\n","    mu_sum = 0\n","    cov_sum = 0\n","    gamma_sum = 0\n","\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n","      \n","      batch_gamma_sum = torch.sum(gamma, dim=0)\n","      \n","      gamma_sum += batch_gamma_sum\n","      mu_sum += mu * batch_gamma_sum.unsqueeze(-1) # keep sums of the numerator only\n","      cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1) # keep sums of the numerator only\n","      \n","      N += input_data.size(0)\n","        \n","    train_phi = gamma_sum / N\n","    train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n","    train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n","\n","    # print(\"N:\",N)\n","    # print(\"phi :\\n\",train_phi)\n","    # print(\"mu :\\n\",train_mu)\n","    # print(\"cov :\\n\",train_cov)\n","\n","    train_energy = []\n","    train_labels = []\n","    train_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      \n","      train_energy.append(sample_energy.data.cpu().numpy())\n","      train_z.append(z.data.cpu().numpy())\n","      # train_labels.append(labels.numpy())\n","\n","    train_energy = np.concatenate(train_energy,axis=0)\n","    train_z = np.concatenate(train_z,axis=0)\n","    # train_labels = np.concatenate(train_labels,axis=0)\n","\n","    ### use validation data\n","    valid_energy = []\n","    valid_labels = []\n","    valid_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader_v):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, size_average=False)\n","      valid_energy.append(sample_energy.data.cpu().numpy())\n","      valid_z.append(z.data.cpu().numpy())\n","      # valid_labels.append(labels.numpy())\n","\n","    valid_energy = np.concatenate(valid_energy,axis=0)\n","    valid_z = np.concatenate(valid_z,axis=0)\n","    # valid_labels = np.concatenate(valid_labels,axis=0)\n","\n","    ### use test data\n","    test_energy = []\n","    test_labels = []\n","    test_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader_test):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, size_average=False)\n","      test_energy.append(sample_energy.data.cpu().numpy())\n","      test_z.append(z.data.cpu().numpy())\n","      # test_labels.append(labels.numpy())\n","\n","    test_energy = np.concatenate(test_energy,axis=0)\n","    test_z = np.concatenate(test_z,axis=0)\n","    # test_labels = np.concatenate(test_labels,axis=0)\n","\n","    # combine\n","    combined_energy = np.concatenate([train_energy, valid_energy, test_energy], axis=0)\n","    # combined_labels = np.concatenate([train_labels, valid_labels, test_labels], axis=0)\n","\n","    thresh = np.percentile(combined_energy, 100 - 0.3)\n","    # print(\"Threshold :\", thresh)\n","\n","    pred = (test_energy > thresh).astype(int)\n","    # gt = test_labels.astype(int)\n","\n","    # accuracy = accuracy_score(gt,pred)\n","    # precision, recall, f_score, support = prf(gt, pred, average='binary')\n","    # print(\"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(accuracy, precision, recall, f_score))\n","    # return accuracy, precision, recall, f_score\n","    return pred\n"],"metadata":{"id":"pFAeYuR7qUhj","executionInfo":{"status":"ok","timestamp":1658908928582,"user_tz":-540,"elapsed":2417,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import torchvision\n","from torch.autograd import Variable\n","import itertools\n","# from utils import *\n","\n","class Cholesky(torch.autograd.Function):\n","  def forward(ctx, a):\n","    # l = torch.potrf(a, False)\n","    l = torch.cholesky(a, False)\n","    ctx.save_for_backward(l)\n","    return l\n","  def backward(ctx, grad_output):\n","    # l, = ctx.saved_variables\n","    l, = ctx.saved_tensors\n","    linv = l.inverse()\n","    inner = torch.tril(torch.mm(l.t(), grad_output)) * torch.tril(\n","        1.0 - Variable(l.data.new(l.size(1)).fill_(0.5).diag()))\n","    s = torch.mm(linv.t(), torch.mm(inner, linv))\n","    return s\n","    \n","class DaGMM(nn.Module):\n","  \"\"\"Residual Block.\"\"\"\n","  def __init__(self, device, n_gmm = 2, latent_dim=3):\n","    super(DaGMM, self).__init__()\n","    self.device = device\n","\n","    # layers = []\n","    # layers += [nn.Linear(30, 20)]\n","    # layers += [nn.BatchNorm1d(20)]\n","    # layers += [nn.LeakyReLU()]\n","    # layers += [nn.Linear(20, 1)]\n","    # layers += [nn.BatchNorm1d(1)]\n","    # layers += [nn.LeakyReLU()]\n","    # self.encoder = nn.Sequential(*layers)\n","  \n","    # layers = []\n","    # layers += [nn.Linear(1, 20)]\n","    # layers += [nn.BatchNorm1d(20)]\n","    # layers += [nn.LeakyReLU()]\n","    # layers += [nn.Linear(20,30)]\n","    # self.decoder = nn.Sequential(*layers)\n","\n","    layers = []\n","    layers += [nn.Linear(30,60)]\n","    # layers += [nn.BatchNorm1d(60)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Linear(60,30)]\n","    # layers += [nn.BatchNorm1d(30)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Linear(30,10)]\n","    # layers += [nn.BatchNorm1d(10)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Linear(10,1)]\n","    self.encoder = nn.Sequential(*layers)\n","\n","    layers = []\n","    layers += [nn.Linear(1,10)]\n","    # layers += [nn.BatchNorm1d(10)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Linear(10,30)]\n","    # layers += [nn.BatchNorm1d(30)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Linear(30,60)]\n","    # layers += [nn.BatchNorm1d(60)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Linear(60,30)]\n","    self.decoder = nn.Sequential(*layers)\n","\n","    layers = []\n","    layers += [nn.Linear(latent_dim,10)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Dropout(p=0.5)]        \n","    layers += [nn.Linear(10,n_gmm)]\n","    layers += [nn.Softmax(dim=1)]\n","    self.estimation = nn.Sequential(*layers)\n","\n","    self.register_buffer(\"phi\", torch.zeros(n_gmm))\n","    self.register_buffer(\"mu\", torch.zeros(n_gmm,latent_dim))\n","    self.register_buffer(\"cov\", torch.zeros(n_gmm,latent_dim,latent_dim))\n","\n","  def relative_euclidean_distance(self, a, b):\n","    return (a-b).norm(2, dim=1) / a.norm(2, dim=1)\n","\n","  def forward(self, x):\n","    enc = self.encoder(x)\n","    dec = self.decoder(enc)\n","\n","    rec_cosine = F.cosine_similarity(x, dec, dim=1)\n","    rec_euclidean = self.relative_euclidean_distance(x, dec)\n","    # z = torch.cat([rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n","    z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n","    \n","    gamma = self.estimation(z)\n","    return enc, dec, z, gamma\n","\n","  def compute_gmm_params(self, z, gamma):\n","    N = gamma.size(0)\n","    sum_gamma = torch.sum(gamma, dim=0) # K\n","    phi = (sum_gamma / N) # K\n","    self.phi = phi.data\n","\n","    mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1) # K x D\n","    self.mu = mu.data\n","    # z = N x D\n","    # mu = K x D\n","    # gamma N x K\n","\n","    z_mu = (z.unsqueeze(1)- mu.unsqueeze(0)) # z_mu = N x K x D\n","    z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2) # z_mu_outer = N x K x D x D   \n","    cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim = 0) / sum_gamma.unsqueeze(-1).unsqueeze(-1) # K x D x D\n","    self.cov = cov.data\n","    return phi, mu, cov\n","      \n","  def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n","    if phi is None : phi = self.phi #self.to_var(self.phi)\n","    if mu is None : mu = self.mu #self.to_var(self.mu)\n","    if cov is None : cov = self.cov # self.to_var(self.cov)\n","\n","    k, D, _ = cov.size()\n","    z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n","\n","    cov_inverse = []\n","    det_cov = []\n","    cov_diag = 0\n","    eps = 1e-12\n","    for i in range(k):\n","      cov_k = cov[i] + (torch.eye(D)*eps).to(device=self.device) # K x D x D\n","      cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n","      \n","      det_cov.append((Cholesky.apply(cov_k.cpu() * (2*np.pi)).diag().prod()).unsqueeze(0))\n","      cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n","\n","    cov_inverse = torch.cat(cov_inverse, dim=0) # K x D x D\n","    det_cov = torch.cat(det_cov).cuda() # K\n","    exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1) # N x \n","    max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0] # for stability (logsumexp)\n","    exp_term = torch.exp(exp_term_tmp - max_val)\n","\n","    sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt(det_cov)).unsqueeze(0), dim = 1) + eps)\n","\n","    if size_average : sample_energy = torch.mean(sample_energy)\n","    return sample_energy, cov_diag\n","\n","  def to_var(x, volatile=False):\n","    if torch.cuda.is_available() : x = x.cuda()\n","    # return torch.tensor(x)\n","    return Variable(x, volatile=volatile)\n","\n","  def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n","    recon_error = torch.mean((x - x_hat) ** 2)\n","    phi, mu, cov = self.compute_gmm_params(z, gamma)\n","    sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n","    loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n","    return loss, sample_energy, recon_error, cov_diag\n","\n","\n","class MyDataset(Dataset):\n","  def __init__(self, df, eval_mode):\n","    self.df = df\n","    self.eval_mode = eval_mode\n","    if self.eval_mode:\n","      self.labels = self.df['Class'].values\n","      self.df = self.df.drop(columns=['Class']).values\n","    else:\n","      self.df = self.df.values\n","      \n","  def __getitem__(self, index):\n","    if self.eval_mode:\n","      self.x = self.df[index]\n","      self.y = self.labels[index]\n","      return torch.Tensor(self.x), self.y\n","    else:\n","      self.x = self.df[index]\n","      return torch.Tensor(self.x), 0\n","  \n","  def __len__(self):\n","    return len(self.df)\n","\n","# train_dataset = MyDataset(df=train_df, eval_mode=False)\n","# train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n","\n","# val_dataset = MyDataset(df = val_df, eval_mode=True)\n","# val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False)\n","\n","import pandas as pd\n","def get_loader(data_path, batch_size, mode='train'):\n","  \"\"\"Build and return data loader.\"\"\"\n","  train_df = pd.read_csv('./train.csv')\n","  train_df = train_df.drop(columns=['ID'])\n","  val_df = pd.read_csv('./val.csv')\n","  val_df = val_df.drop(columns=['ID'])\n","  test_df = pd.read_csv('./test.csv')\n","  test_df = test_df.drop(columns=['ID'])\n","\n","  #-------------------#\n","  #---# Normalize #---#\n","  #-------------------#\n","  from sklearn.preprocessing import StandardScaler\n","  # case 1 - standardscaler\n","  scaler_n = StandardScaler()\n","  scaler_n.fit(train_df.values)\n","  \n","  val_x = val_df.drop(columns=['Class'])\n","  train_x_scaleN = pd.DataFrame(scaler_n.transform(train_df.values), columns = train_df.columns) # 확인 : train_x_scaleN.mean(), train_x_scaleN.var()\n","  val_x_scaleN = pd.DataFrame(scaler_n.transform(val_x.values), columns = val_x.columns)\n","  test_x_scaleN = pd.DataFrame(scaler_n.transform(test_df.values), columns = test_df.columns)\n","\n","  train_df = train_x_scaleN\n","  val_df = pd.concat([val_x_scaleN, pd.DataFrame(val_df['Class'])], axis=1)\n","  test_df = test_x_scaleN\n","  \n","  # dataset = MyDataset(data_path, mode)\n","  train_dataset = MyDataset(df=train_df, eval_mode=False)\n","  valid_dataset = MyDataset(df=val_df, eval_mode=True)\n","  test_dataset = MyDataset(df=test_df, eval_mode=False)\n","  \n","  shuffle = False\n","  if mode == 'train': shuffle = True\n","\n","  data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=shuffle)\n","  data_loader_v = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=shuffle)\n","  data_loader_test = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=shuffle)\n","  return data_loader, data_loader_v, data_loader_test"],"metadata":{"id":"Y2l-jtBmPbIS","executionInfo":{"status":"ok","timestamp":1658908929336,"user_tz":-540,"elapsed":762,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import os\n","import argparse\n","# from solver import Solver\n","# from data_loader import get_loader\n","from torch.backends import cudnn\n","# from utils import *\n","import torch\n","def str2bool(v):\n","  return v.lower() in ('true')\n","\n","def main(config):\n","  # For fast training\n","  cudnn.benchmark = True\n","  wandb.init()\n","  data_loader, data_loader_v, data_loader_test = get_loader(config.data_path, batch_size=config.batch_size, mode=config.mode)\n","  USE_CUDA = torch.cuda.is_available()\n","  device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n","\n","  # Solver\n","  solver = Solver(device, data_loader, data_loader_v, data_loader_test, vars(config))\n","\n","  if config.mode == 'train' : \n","    solver.train()\n","    return solver, 0\n","  elif config.mode == 'test' : \n","    pred = solver.test()\n","    return solver, pred\n","\n","if __name__ == '__main__':\n","  import easydict\n","  args = easydict.EasyDict({\n","      \"num_epochs\" : 100,\n","      \"batch_size\" : 8192,\n","      \"gmm_k\" : 2,\n","      \"lambda_energy\" : 0.1,\n","      \"lambda_cov_diag\" : 0.005,\n","      # \"pretrained_model\" : '',\n","      \"pretrained_model\" : None,\n","      \"mode\" : 'train',\n","      # \"mode\" : \"test\",\n","      \"data_path\" : \"./\",   \n","      \"use_tensorboard\" : False,\n","      \"log_path\" : './logs',\n","      \"model_save_path\" : './models',\n","      \"log_step\" : 100,\n","      \"sample_step\" : 194,\n","      \"model_save_step\" : 194,\n","      \"lr\" : 1e-4,\n","      \"wd\" : None\n","  })\n","  config = args\n","  # args = vars(config)\n","  print('------------ Options -------------')\n","  for k, v in sorted(args.items()):\n","    print('%s: %s' % (str(k), str(v)))\n","  print('-------------- End ----------------')\n","\n","  solver, pred = main(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sDr1-i0dQUPl","executionInfo":{"status":"error","timestamp":1658908886891,"user_tz":-540,"elapsed":2493,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"19d7a01c-6f8e-421f-a7e6-c61e94c5eb40"},"execution_count":134,"outputs":[{"output_type":"stream","name":"stdout","text":["------------ Options -------------\n","batch_size: 8192\n","data_path: ./\n","gmm_k: 2\n","lambda_cov_diag: 0.005\n","lambda_energy: 0.1\n","log_path: ./logs\n","log_step: 100\n","lr: 0.0001\n","mode: train\n","model_save_path: ./models\n","model_save_step: 194\n","num_epochs: 100\n","pretrained_model: None\n","sample_step: 194\n","use_tensorboard: False\n","wd: None\n","-------------- End ----------------\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\", line 1040, in init\n","    wi.setup(kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\", line 186, in setup\n","    tel.feature.set_init_tags = True\n","  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/telemetry.py\", line 41, in __exit__\n","    self._run._telemetry_callback(self._obj)\n","  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\", line 572, in _telemetry_callback\n","    self._telemetry_flush()\n","  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\", line 583, in _telemetry_flush\n","    self._backend.interface._publish_telemetry(self._telemetry_obj)\n","  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_shared.py\", line 74, in _publish_telemetry\n","    self._publish(rec)\n","  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_queue.py\", line 49, in _publish\n","    raise Exception(\"The wandb backend process has shutdown\")\n","Exception: The wandb backend process has shutdown\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n"]},{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tags\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mtel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_init_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/telemetry.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exctype, excinst, exctb)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_telemetry_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_telemetry_callback\u001b[0;34m(self, telem_obj)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_telemetry_obj_dirty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_telemetry_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_telemetry_flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_telemetry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_telemetry_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_telemetry_obj_flushed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_telemetry\u001b[0;34m(self, telem)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtelemetry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtelem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_queue.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-134-4bd999ce94fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------- End ----------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-134-4bd999ce94fe>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# For fast training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mUSE_CUDA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexcept_exit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"problem\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merror_seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: problem"]}]},{"cell_type":"code","source":["sum(pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L0yN_tuBLCPJ","executionInfo":{"status":"ok","timestamp":1658887135521,"user_tz":-540,"elapsed":319,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"53ab2ddf-8b21-40cd-fe0b-4cde9b38f413"},"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["839"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["pred\n","submit = pd.read_csv('./sample_submission.csv')\n","submit['Class'] = pred\n","submit.to_csv('./submit_DAGMM_0.5_20220727.csv', index=False)"],"metadata":{"id":"KZ30QcJKk1j6","executionInfo":{"status":"ok","timestamp":1658881826317,"user_tz":-540,"elapsed":288,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["pred"],"metadata":{"id":"IT9pjS56k1bW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658881837105,"user_tz":-540,"elapsed":307,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"4a02c40d-ea90-496e-9bf6-d5c66d5729dc"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, ..., 0, 0, 0])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["sum(pred)"],"metadata":{"id":"RRB3q6iOToQH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658881842417,"user_tz":-540,"elapsed":6,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"3698cb26-18fb-45c5-bbff-965698ca60c5"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1086"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":[""],"metadata":{"id":"YdlTEqmoQTJV"},"execution_count":null,"outputs":[]}]}