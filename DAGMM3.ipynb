{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DAGMM3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPLUdDEkY9CyOZzG3OofSuW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JhbVmM5RIap-","executionInfo":{"status":"ok","timestamp":1658732088576,"user_tz":-540,"elapsed":18952,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"00a22b49-5c54-47ac-855b-0d65ec4fe3b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n","/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n"]}],"source":["from google.colab import drive\n","# drive.mount('/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data')\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n","!pwd"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import os\n","import time\n","import datetime\n","from torch.autograd import grad\n","from torch.autograd import Variable\n","# from model import *\n","import matplotlib.pyplot as plt\n","# from utils import *\n","# from data_loader import *\n","import IPython\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","import itertools\n","# from utils import *\n","\n","class EarlyStopping:\n","  def __init__(self, patience=30):\n","      self.loss = np.inf\n","      self.patience = 0\n","      self.patience_limit = patience\n","      \n","  def step(self, loss):\n","      if self.loss > loss:\n","          self.loss = loss\n","          self.patience = 0\n","      else:\n","          self.patience += 1\n","  \n","  def is_stop(self):\n","      return self.patience >= self.patience_limit"],"metadata":{"id":"5M6ZwVdQIfAy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    def __init__(self, df, eval_mode):\n","        self.df = df\n","        self.eval_mode = eval_mode\n","        if self.eval_mode:\n","            self.labels = self.df['Class'].values\n","            self.df = self.df.drop(columns=['Class']).values\n","        else:\n","            self.df = self.df.values\n","        \n","    def __getitem__(self, index):\n","        if self.eval_mode :\n","            self.x = self.df[index]\n","            self.y = self.labels[index]\n","            return torch.Tensor(self.x), self.y\n","        else:\n","            self.x = self.df[index]\n","            return torch.Tensor(self.x), 0\n","        \n","    def __len__(self):\n","        return len(self.df)\n","\n","# train_dataset = MyDataset(df=train_df, eval_mode=False)\n","# train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n","\n","# val_dataset = MyDataset(df = val_df, eval_mode=True)\n","# val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False)\n","\n","\n","\"\"\"Build and return data loader.\"\"\"\n","train_df = pd.read_csv('./train.csv')\n","train_df = train_df.drop(columns=['ID'])\n","val_df = pd.read_csv('./val.csv')\n","val_df = val_df.drop(columns=['ID'])\n","test_df = pd.read_csv('./test.csv')\n","test_df = test_df.drop(columns=['ID'])\n","\n","#-------------------#\n","#---# Normalize #---#\n","#-------------------#\n","# case 1 - standardscaler\n","scaler_n = StandardScaler()\n","scaler_n.fit(train_df.values)\n","\n","val_x = val_df.drop(columns=['Class'])\n","train_x_scaleN = pd.DataFrame(scaler_n.transform(train_df.values), columns = train_df.columns) # 확인 : train_x_scaleN.mean(), train_x_scaleN.var()\n","val_x_scaleN = pd.DataFrame(scaler_n.transform(val_x.values), columns = val_x.columns)\n","test_x_scaleN = pd.DataFrame(scaler_n.transform(test_df.values), columns = test_df.columns)\n","\n","train_df = train_x_scaleN\n","val_df = pd.concat([val_x_scaleN, pd.DataFrame(val_df['Class'])])\n","test_df = test_x_scaleN\n","\n","# dataset = MyDataset(data_path, mode)\n","train_dataset = MyDataset(df=train_df, eval_mode=False)\n","valid_dataset = MyDataset(df=val_df, eval_mode=True)\n","test_dataset = MyDataset(df=test_df, eval_mode=False)\n","\n","shuffle = False\n","if mode == 'train': shuffle = True\n","\n","data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=shuffle)\n","data_loader_v = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=shuffle)\n","data_loader_test = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=shuffle)\n","return data_loader, data_loader_v, data_loader_test"],"metadata":{"id":"_BW5OKBJIflY"},"execution_count":null,"outputs":[]}]}