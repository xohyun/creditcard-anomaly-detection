{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AutoEncoder+GMM.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"13lR3U5j_ghZfWK6Ylw_UeSrglnpswhwf","authorship_tag":"ABX9TyPU8NCV6PbjqUKXXbIKvqwc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","# drive.mount('/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data')\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/IITP/sohyun/creditcard_prediction/data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aoTfHzuB_M7u","executionInfo":{"status":"ok","timestamp":1658103698292,"user_tz":-540,"elapsed":10243,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"1f9ad365-a215-4216-fd82-c216f0b88002"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n"]}]},{"cell_type":"code","source":["!pip install wandb -qqq\n","import wandb\n","wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"id":"pyCrbam-X2js","executionInfo":{"status":"ok","timestamp":1657777438975,"user_tz":-540,"elapsed":14750,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"0b59097f-99ad-4304-a35e-cc7ce842d6eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 1.8 MB 7.4 MB/s \n","\u001b[K     |████████████████████████████████| 181 kB 56.2 MB/s \n","\u001b[K     |████████████████████████████████| 146 kB 68.0 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.ensemble import IsolationForest\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(action='ignore')\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.decomposition import PCA\n","\n","# seed 고정\n","import random\n","random.seed(1004)\n","\n","#-------------------#\n","#---# Data Load #---#\n","#-------------------#\n","train_df = pd.read_csv('./train.csv') # Train\n","val_df = pd.read_csv('./val.csv') # Validation\n","test_df = pd.read_csv('./test.csv') # Test\n","\n","# validation set 사기 거래 비율\n","val_normal, val_fraud = val_df['Class'].value_counts()\n","val_contamination = val_fraud / val_normal\n","\n","train_x = train_df.drop(columns=['ID']) # Input Data # Train dataset은 Label이 존재하지 않음\n","val_x = val_df.drop(columns=['ID', 'Class']) # Input Data\n","val_y = val_df['Class'] # Label\n","test_x = test_df.drop(columns=['ID'])\n","submit = pd.read_csv('./sample_submission.csv')\n","\n","### train + validation\n","x_t = train_df.drop(columns=['ID']) # Input Data\n","x_v = val_df.drop(columns=['ID']) # Input Data\n","y_v = val_df['Class'] # Label\n","x_t['Class'] = 0\n","tv = pd.concat([x_t, x_v]) # train + validation dataset (train label은 0으로 우선 넣음)\n","tv = tv.reset_index(drop = True)\n","y_tv = tv['Class']\n","x_tv = tv.drop(columns=['Class'])\n","\n","#-------------------#\n","#---# Normalize #---#\n","#-------------------#\n","# case 1 - standardscaler\n","scaler_n = StandardScaler()\n","scaler_n.fit(train_x)\n","\n","train_x_scaleN = pd.DataFrame(scaler_n.transform(train_x), columns = train_x.columns) # 확인 : train_x_scaleN.mean(), train_x_scaleN.var()\n","val_x_scaleN = pd.DataFrame(scaler_n.transform(val_x), columns = val_x.columns)\n","test_x_scaleN = pd.DataFrame(scaler_n.transform(test_x), columns = test_x.columns)\n","\n","# case 2 - minmax scaler\n","scaler_m = MinMaxScaler()\n","scaler_m.fit(train_x)\n","\n","train_x_scaleM = pd.DataFrame(scaler_m.transform(train_x), columns = train_x.columns)\n","val_x_scaleM = pd.DataFrame(scaler_m.transform(val_x), columns = val_x.columns)\n","test_x_scaleM = pd.DataFrame(scaler_m.transform(test_x), columns = test_x.columns)\n","\n","# train + validation - case 1\n","scaler_n_all = StandardScaler()\n","scaler_n_all.fit(x_tv)\n","x_scaleN = pd.DataFrame(scaler_n_all.transform(x_tv), columns = x_tv.columns) # scaler_n_all.transform(x_tv) : 결과 ndarray\n","\n","# train + validation - case 2\n","scaler_m_all = MinMaxScaler()\n","scaler_m_all.fit(x_tv)\n","x_scaleM = pd.DataFrame(scaler_m_all.transform(x_tv), columns = x_tv.columns)\n","\n","\n","#-----------------------------#\n","#---# Dimension Reduction #---#\n","#-----------------------------#\n","n_pca = 5\n","# pca = PCA() # n_componenets : 투영할 차원의 수\n","# pca.fit(train_x_scaleN)\n","# train_x_pca_scaleN = pd.DataFrame(pca.transform(train_x_scaleN), columns = train_x.columns)\n","# val_x_pca_scaleN = pd.DataFrame(pca.transform(val_x_scaleN), columns = val_x.columns)\n","# test_x_pca_scaleN = pd.DataFrame(pca.transform(test_x_scaleN), columns = test_x.columns)\n","\n","# pca2 = PCA()\n","# pca2.fit(train_x_scaleM)\n","# train_x_pca_scaleM = pd.DataFrame(pca2.transform(train_x_scaleM), columns = train_x.columns)\n","# val_x_pca_scaleM = pd.DataFrame(pca2.transform(val_x_scaleM), columns = val_x.columns)\n","# test_x_pca_scaleM = pd.DataFrame(pca2.transform(test_x_scaleM), columns = test_x.columns)\n","\n","# train + validation\n","pca3 = PCA(n_pca)\n","pca3.fit(x_scaleN)\n","x_pca_scaleN = pd.DataFrame(pca3.transform(x_scaleN))\n","test_x_pca_scaleN_all = pd.DataFrame(pca3.transform(test_x_scaleN)) ###\n","\n","pca4 = PCA(n_pca)\n","pca4.fit(x_scaleM)\n","x_pca_scaleM = pd.DataFrame(pca4.transform(x_scaleM))\n","test_x_pca_scaleM_all = pd.DataFrame(pca4.transform(test_x_scaleM))\n","\n","from sklearn.mixture import GaussianMixture\n","\n","N_mixture = 8\n","gmm = GaussianMixture(n_components=N_mixture, random_state = 1004, covariance_type = 'full').fit(x_pca_scaleN)\n","gmm_labels = gmm.predict(x_pca_scaleN)\n","\n","print(f\"0의 개수 : {list(gmm_labels).count(0)}, 1의 개수 : {list(gmm_labels).count(1)}\")\n","\n","# means : 평균 / cov : 공분산 / std : 표준편차\n","means = gmm.means_\n","cov = gmm.covariances_\n","# std = [np.sqrt(np.trace(cov[i]/N_mixture)) for i in range(0, N_mixture)]\n","std = []\n","for i in range(len(cov)) :\n","  each_g = []\n","  for j in range(n_pca) :\n","    each_g.append(np.sqrt(cov[i][j, j]))\n","  std.append(each_g)\n","\n","anomal_idx = list(y_tv == 1)\n","nomal_idx_ = list(y_tv == 0)\n","\n","anomal_x = x_pca_scaleN.loc[anomal_idx, :]\n","anomal_x = anomal_x.reset_index(drop=True)\n","# anomal_x = anomal_x.drop(columns=['index'])\n","\n","probability = gmm.predict_proba(anomal_x) # Evaluate the components’ density for each sample.\n","print(pd.DataFrame(probability).idxmax(axis = 1).value_counts())\n","print(pd.DataFrame(probability).idxmax(axis = 1).value_counts().idxmax()) \n","outlier_dist = list(pd.DataFrame(probability).idxmax(axis = 1).value_counts().nlargest(1).index) # 가장 많은 이상값을 가진 분포\n","print(f\"outlier_dist:{outlier_dist}\")\n","## test\n","# p_test = gmm.predict_proba(test_x_pca_scaleN_all)\n","# p_test_df = pd.DataFrame(p_test).idxmax(axis = 1).to_frame()\n","# out_dist = list(pd.DataFrame(probability).idxmax(axis = 1).value_counts().nlargest(3).index)\n","\n","p_train = gmm.predict_proba(x_pca_scaleN)\n","p_train_df = pd.DataFrame(p_train).idxmax(axis = 1).to_frame()\n","\n","only_normal = p_train_df[~p_train_df.loc[:,0].isin(outlier_dist)] \n","# p_df['Class'] = np.where(p_df[[0]] == 6, 1, 0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mAG00KfroM6m","executionInfo":{"status":"ok","timestamp":1658103729773,"user_tz":-540,"elapsed":29130,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"6d4f3fea-9517-4aa0-dfbd-a45450301547"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["0의 개수 : 36951, 1의 개수 : 31175\n","2    15\n","4     7\n","7     3\n","3     3\n","0     1\n","1     1\n","dtype: int64\n","2\n","outlier_dist:[2]\n"]}]},{"cell_type":"code","source":["# p_df.loc[:,0].isin(out_dist)\n","only_normal.loc[:,0] = 0 \n","only_normal_idx = only_normal.index.tolist()\n","train_df = x_tv.loc[only_normal_idx,:]"],"metadata":{"id":"SW5soc_7VXMu","executionInfo":{"status":"ok","timestamp":1658103731937,"user_tz":-540,"elapsed":380,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gPauNGcl-kic","executionInfo":{"status":"ok","timestamp":1658103736251,"user_tz":-540,"elapsed":2304,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":["import random\n","import pandas as pd\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","EPOCHS = 500\n","LR = 1e-2\n","BS = 15000 # 16384\n","SEED = 1004\n","\n","# wandb.init(project=\"\") # wandb init\n","\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(SEED) # Seed 고정\n","\n","# train_df = pd.read_csv('./train.csv')\n","# train_df = train_df.drop(columns=['ID'])\n","val_df = pd.read_csv('./val.csv')\n","val_df = val_df.drop(columns=['ID'])"]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    def __init__(self, df, eval_mode):\n","        self.df = df\n","        self.eval_mode = eval_mode\n","        if self.eval_mode:\n","            self.labels = self.df['Class'].values\n","            self.df = self.df.drop(columns=['Class']).values\n","        else:\n","            self.df = self.df.values\n","        \n","    def __getitem__(self, index):\n","        if self.eval_mode:\n","            self.x = self.df[index]\n","            self.y = self.labels[index]\n","            return torch.Tensor(self.x), self.y\n","        else:\n","            self.x = self.df[index]\n","            return torch.Tensor(self.x)\n","        \n","    def __len__(self):\n","        return len(self.df)\n","\n","train_dataset = MyDataset(df=train_df, eval_mode=False)\n","train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n","\n","val_dataset = MyDataset(df = val_df, eval_mode=True)\n","val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False)\n","\n","\n","class AutoEncoder(nn.Module):\n","    def __init__(self):\n","        super(AutoEncoder, self).__init__()\n","        self.Encoder = nn.Sequential(\n","            nn.Linear(30,64),\n","            nn.BatchNorm1d(64),\n","            nn.LeakyReLU(),\n","            nn.Linear(64,128),\n","            nn.BatchNorm1d(128),\n","            nn.LeakyReLU(),\n","            # nn.Linear(30,20),\n","            # nn.BatchNorm1d(20),\n","            # nn.ReLU(),\n","\n","            # nn.Linear(20,10),\n","            # nn.BatchNorm1d(10),\n","            # nn.ReLU(),\n","\n","            # nn.Linear(10,5),\n","            # nn.BatchNorm1d(5),\n","            # nn.ReLU()\n","\n","        #     nn.Linear(64,128),\n","        #     nn.BatchNorm1d(128),\n","        #     nn.LeakyReLU(),\n","\n","        )\n","        self.Decoder = nn.Sequential(\n","            nn.Linear(128,64),\n","            nn.BatchNorm1d(64),\n","            nn.LeakyReLU(),\n","            nn.Linear(64,30),\n","            # nn.Linear(5,10),\n","            # nn.BatchNorm1d(10),\n","            # nn.ReLU(),\n","            # nn.Linear(10,20),\n","            # nn.BatchNorm1d(20),\n","            # nn.ReLU(),\n","            # nn.Linear(20,30)\n","\n","            # nn.Linear(128,64),\n","            # nn.BatchNorm1d(64),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(64,30),\n","        )\n","        \n","    def forward(self, x, device):\n","        x_enc = self.Encoder(x)\n","        x_dec = self.Decoder(x_enc)\n","        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n","        diff = cos(x, x_dec).cpu().tolist()\n","        diff = torch.tensor(diff, device = device) # .reshape(-1, 1)\n","        print(x.shape)\n","        print(diff.shape)\n","        concat_vector = torch.stack([x.reshape(-1,1).squeeze(), diff], dim = 0)\n","        return x, concat_vector\n","\n","class Trainer():\n","    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.scheduler = scheduler\n","        self.device = device\n","        # Loss Function\n","        self.criterion = nn.L1Loss().to(self.device)\n","        # self.criterion = nn.MSELoss().to(self.device)\n","    \n","    def fit(self):\n","        self.model.to(self.device)\n","        best_score = 0\n","        for epoch in range(EPOCHS):\n","            self.model.train()\n","            train_loss = []\n","            for x in iter(self.train_loader):\n","                x = x.float().to(self.device)\n","                self.optimizer.zero_grad()\n","\n","                _x = self.model(x, self.device)\n","                loss = self.criterion(x, _x)\n","\n","                loss.backward()\n","                self.optimizer.step()\n","                train_loss.append(loss.item())\n","\n","            score = self.validation(self.model, 0.95)\n","\n","            # wandb.log({\"f1-score\":score, \"train_loss\":loss})\n","\n","            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n","\n","            if self.scheduler is not None:\n","                self.scheduler.step(score)\n","\n","            if best_score < score:\n","                best_score = score\n","                torch.save(model.module.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n","    \n","    def validation(self, eval_model, thr):\n","        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n","        eval_model.eval()\n","        pred = []\n","        true = []\n","        with torch.no_grad():\n","            for x, y in iter(self.val_loader):\n","                x = x.float().to(self.device)\n","\n","                _x = self.model(x, self.device)\n","                diff = cos(x, _x).cpu().tolist()\n","                batch_pred = np.where(np.array(diff)<thr,1,0).tolist()\n","                pred += batch_pred\n","                true += y.tolist()\n","\n","        return f1_score(true, pred, average='macro')\n","\n","model = nn.DataParallel(AutoEncoder())\n","model.eval()\n","optimizer = torch.optim.Adam(params = model.parameters(), lr = LR)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n","\n","trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n","trainer.fit()\n","\n","model = AutoEncoder()\n","model.load_state_dict(torch.load('./best_model.pth'))\n","model = nn.DataParallel(model)\n","model.eval()\n","\n","test_df = pd.read_csv('./test.csv')\n","test_df = test_df.drop(columns=['ID'])\n","\n","test_dataset = MyDataset(test_df, False)\n","test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=6)\n","\n","def prediction(model, thr, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n","    pred = []\n","    with torch.no_grad():\n","        for x in iter(test_loader):\n","            x = x.float().to(device)\n","            \n","            _x = model(x)\n","            \n","            diff = cos(x, _x).cpu().tolist()\n","            batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n","            pred += batch_pred\n","    return pred\n","\n","preds = prediction(model, 0.95, test_loader, device)"],"metadata":{"id":"kijk5u73jo-B","colab":{"base_uri":"https://localhost:8080/","height":435},"executionInfo":{"status":"error","timestamp":1658107914336,"user_tz":-540,"elapsed":549,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"4da8d738-54c9-4b94-e396-4c9c61c529d1"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([15000, 30])\n","torch.Size([15000])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-0017cab40acb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-0017cab40acb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-0017cab40acb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mconcat_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [450000] at entry 0 and [15000] at entry 1"]}]},{"cell_type":"code","source":["submit = pd.read_csv('./sample_submission22.csv')\n","submit['Class'] = preds\n","submit.to_csv('./submit_autoencoder.csv', index=False)"],"metadata":{"id":"IgdZWXD0IJkR","executionInfo":{"status":"aborted","timestamp":1658106735756,"user_tz":-540,"elapsed":15,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mSenGm1oJFWH"},"execution_count":null,"outputs":[]}]}