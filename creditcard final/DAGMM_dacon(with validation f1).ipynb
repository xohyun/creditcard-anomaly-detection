{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19163,"status":"ok","timestamp":1671007990874,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"},"user_tz":-540},"id":"6Sf4Jnkjo2Lo","outputId":"6de33154-49b3-4048-97a8-4dbd22831440"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n","/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n"]}],"source":["from google.colab import drive\n","# drive.mount('/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data')\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hVhT5DjqRDT"},"outputs":[],"source":["# !pip install wandb -qqq\n","# import wandb\n","# wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUO6B0KmXfC4"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import os\n","import time\n","import datetime\n","from torch.autograd import grad\n","from torch.autograd import Variable\n","# from model import *\n","import matplotlib.pyplot as plt\n","# from utils import *\n","# from data_loader import *\n","import IPython\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","import itertools\n","# from utils import *"]},{"cell_type":"markdown","metadata":{"id":"N1C689jbFrNe"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ovZWagApGNQ3"},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, df, eval_mode):\n","        self.df = df\n","        self.eval_mode = eval_mode\n","        if self.eval_mode:\n","            self.labels = self.df['Class'].values\n","            self.df = self.df.drop(columns=['Class']).values\n","        else:\n","            self.df = self.df.values\n","        \n","    def __getitem__(self, index):\n","        if self.eval_mode :\n","            self.x = self.df[index]\n","            self.y = self.labels[index]\n","            return torch.Tensor(self.x), self.y\n","        else:\n","            self.x = self.df[index]\n","            return torch.Tensor(self.x), 0\n","        \n","    def __len__(self):\n","        return len(self.df)\n","\n","# train_dataset = MyDataset(df=train_df, eval_mode=False)\n","# train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n","\n","# val_dataset = MyDataset(df = val_df, eval_mode=True)\n","# val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False)\n","\n","\n","def get_loader(data_path, batch_size, mode='train'):\n","    \"\"\"Build and return data loader.\"\"\"\n","    train_df = pd.read_csv('./train.csv')\n","    train_df = train_df.drop(columns=['ID'])\n","    val_df = pd.read_csv('./val.csv')\n","    val_df = val_df.drop(columns=['ID'])\n","    test_df = pd.read_csv('./test.csv')\n","    test_df = test_df.drop(columns=['ID'])\n","\n","    #-------------------#\n","    #---# Normalize #---#\n","    #-------------------#\n","    # case 1 - standardscaler\n","    scaler_n = StandardScaler()\n","    scaler_n.fit(train_df.values)\n","    \n","    val_x = val_df.drop(columns=['Class'])\n","    train_x_scaleN = pd.DataFrame(scaler_n.transform(train_df.values), columns = train_df.columns) # 확인 : train_x_scaleN.mean(), train_x_scaleN.var()\n","    val_x_scaleN = pd.DataFrame(scaler_n.transform(val_x.values), columns = val_x.columns)\n","    test_x_scaleN = pd.DataFrame(scaler_n.transform(test_df.values), columns = test_df.columns)\n","\n","    train_df = train_x_scaleN\n","    val_df = pd.concat([val_x_scaleN, pd.DataFrame(val_df['Class'])])\n","    test_df = test_x_scaleN\n","    \n","    # dataset = MyDataset(data_path, mode)\n","    train_dataset = MyDataset(df=train_df, eval_mode=False)\n","    valid_dataset = MyDataset(df=val_df, eval_mode=True)\n","    test_dataset = MyDataset(df=test_df, eval_mode=False)\n","    \n","    shuffle = False\n","    if mode == 'train': shuffle = True\n","\n","    data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=shuffle)\n","    data_loader_v = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=shuffle)\n","    data_loader_test = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=shuffle)\n","    return data_loader, data_loader_v, data_loader_test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"egBQ3x3AN76P"},"outputs":[],"source":["class EarlyStopping:\n","  def __init__(self, patience=30):\n","      self.loss = np.inf\n","      self.patience = 0\n","      self.patience_limit = patience\n","      \n","  def step(self, loss):\n","      if self.loss > loss:\n","          self.loss = loss\n","          self.patience = 0\n","      else:\n","          self.patience += 1\n","  \n","  def is_stop(self):\n","      return self.patience >= self.patience_limit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFAeYuR7qUhj"},"outputs":[],"source":["class Solver(object):\n","  DEFAULTS = {}   \n","  def __init__(self, device, data_loader, data_loader_v, data_loader_test, config):\n","    # Data loader\n","    self.__dict__.update(Solver.DEFAULTS, **config)\n","    self.data_loader = data_loader\n","    self.data_loader_v = data_loader_v\n","    self.data_loader_test = data_loader_test\n","    self.device = device\n","    self.lr = 1e-4\n","\n","    # Build tensorboard if use\n","    self.build_model()\n","    if self.use_tensorboard:self.build_tensorboard()\n","\n","    # Start with trained model\n","    if self.pretrained_model: self.load_pretrained_model()\n","\n","  def build_model(self):\n","    # Define model\n","    self.dagmm = DaGMM(self.device, self.gmm_k)\n","\n","    # Optimizers\n","    self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr)\n","\n","    # Print networks\n","    self.print_network(self.dagmm, 'DaGMM')\n","\n","    if torch.cuda.is_available():\n","        self.dagmm.cuda()\n","\n","  def print_network(self, model, name):\n","    num_params = 0\n","    for p in model.parameters():\n","        num_params += p.numel()\n","    print(name)\n","    # print(model)\n","    print(\"The number of parameters: {}\".format(num_params))\n","\n","  def load_pretrained_model(self):\n","    self.dagmm.load_state_dict(torch.load(os.path.join(\n","        self.model_save_path, '{}_dagmm.pth'.format(self.pretrained_model))))\n","\n","    print(\"phi\", self.dagmm.phi,\"mu\",self.dagmm.mu, \"cov\",self.dagmm.cov)\n","    print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n","\n","  # def build_tensorboard(self):\n","  #     from logger import Logger\n","  #     self.logger = Logger(self.log_path)\n","\n","  def reset_grad(self):\n","    self.dagmm.zero_grad()\n","\n","  def to_var(self, x, volatile=False):\n","    if torch.cuda.is_available() : x = x.cuda()\n","    return Variable(x, volatile=volatile)\n","\n","  def train(self):\n","    iters_per_epoch = len(self.data_loader)\n","\n","    # Start with trained model if exists\n","    if self.pretrained_model:\n","        start = int(self.pretrained_model.split('_')[0])\n","    else:\n","        start = 0\n","\n","    # Start training\n","    iter_ctr = 0\n","    start_time = time.time()\n","\n","    self.ap_global_train = np.array([0,0,0])\n","    best_f_score = -1\n","    early_stop = EarlyStopping(patience=30) # EarlyStopping\n","    \n","    for e in range(start, self.num_epochs):\n","      progress_bar = tqdm(self.data_loader)\n","      for i, (input_data, labels) in enumerate(progress_bar):\n","        iter_ctr += 1\n","        start = time.time()\n","\n","        input_data = self.to_var(input_data) # cuda로\n","        total_loss, sample_energy, recon_error, cov_diag = self.dagmm_step(input_data)\n","        progress_bar.set_description(f'{e+1} epoch\\'s total_loss : {total_loss.data.item()}')\n","\n","        # Logging\n","        loss = {}\n","        loss['total_loss'] = total_loss.data.item()\n","        loss['sample_energy'] = sample_energy.item()\n","        loss['recon_error'] = recon_error.item()\n","        loss['cov_diag'] = cov_diag.item()\n","\n","        # Print out log info\n","        if (i+1) % self.log_step == 0:\n","          elapsed = time.time() - start_time\n","          total_time = ((self.num_epochs*iters_per_epoch)-(e*iters_per_epoch+i)) * elapsed/(e*iters_per_epoch+i+1)\n","          epoch_time = (iters_per_epoch-i)* elapsed/(e*iters_per_epoch+i+1)\n","          \n","          epoch_time = str(datetime.timedelta(seconds=epoch_time))\n","          total_time = str(datetime.timedelta(seconds=total_time))\n","          elapsed = str(datetime.timedelta(seconds=elapsed))\n","\n","          lr_tmp = []\n","          for param_group in self.optimizer.param_groups:\n","              lr_tmp.append(param_group['lr'])\n","          tmplr = np.squeeze(np.array(lr_tmp))\n","\n","          log = \"Elapsed {}/{} -- {} , Epoch [{}/{}], Iter [{}/{}], lr {}\".format(\n","              elapsed,epoch_time,total_time, e+1, self.num_epochs, i+1, iters_per_epoch, tmplr)\n","\n","          for tag, value in loss.items():\n","              log += \", {}: {:.4f}\".format(tag, value)\n","\n","        # early_stop.step(total_loss.item())\n","        # if early_stop.is_stop() : break\n","        accuracy, precision, recall, f_score = self.validation()\n","        print(\"----\", f_score)\n","\n","  def dagmm_step(self, input_data):\n","    self.dagmm.train()\n","    enc, dec, z, gamma = self.dagmm(input_data)\n","    total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma, self.lambda_energy, self.lambda_cov_diag)\n","    \n","    self.reset_grad()\n","    total_loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(self.dagmm.parameters(), 5)\n","    self.optimizer.step()\n","\n","    return total_loss, sample_energy, recon_error, cov_diag\n","\n","  def validation(self):\n","    self.dagmm.eval()\n","    ########################################################################\n","    ## use train data\n","    N = 0; mu_sum = 0; cov_sum = 0; gamma_sum = 0\n","\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n","      \n","      batch_gamma_sum = torch.sum(gamma, dim=0)\n","      \n","      gamma_sum += batch_gamma_sum\n","      mu_sum += mu * batch_gamma_sum.unsqueeze(-1) # keep sums of the numerator only\n","      cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1) # keep sums of the numerator only\n","      \n","      N += input_data.size(0)\n","        \n","    train_phi = gamma_sum / N\n","    train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n","    train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n","    \n","    # print(\"N:\",N)\n","    # print(\"phi :\\n\",train_phi)\n","    # print(\"mu :\\n\",train_mu)\n","    # print(\"cov :\\n\",train_cov)\n","\n","    train_energy = []\n","    train_labels = []\n","    train_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      \n","      train_energy.append(sample_energy.data.cpu().numpy())\n","      train_z.append(z.data.cpu().numpy())\n","      # train_labels.append(labels.numpy())\n","\n","    train_energy = np.concatenate(train_energy,axis=0)\n","    train_z = np.concatenate(train_z,axis=0)\n","    # train_labels = np.concatenate(train_labels,axis=0)\n","\n","    ########################################################################\n","    ## validation data\n","    val_energy = []; val_labels = []; val_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader_v):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      val_energy.append(sample_energy.data.cpu().numpy())\n","      val_z.append(z.data.cpu().numpy())\n","      val_labels.append(labels.numpy())\n","\n","    val_energy = np.concatenate(val_energy,axis=0)\n","    val_z = np.concatenate(val_z,axis=0)\n","    val_labels = np.concatenate(val_labels,axis=0)\n","\n","    combined_energy = np.concatenate([train_energy, val_energy], axis=0)\n","    print(\"<<<<<<<<<\", len(combined_energy))\n","    # combined_labels = np.concatenate([train_labels, val_labels, test_labels], axis=0)\n","    print(np.percentile(train_energy, 90), np.percentile(val_energy, 90))\n","    thresh = np.percentile(combined_energy * 10000, 100 - 0.01) / 10000\n","    print(\"Threshold :\", thresh)\n","\n","    pred = (val_energy > thresh).astype(int)\n","    gt = val_labels.astype(int)\n","\n","    from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n","    accuracy = accuracy_score(gt,pred)\n","\n","    precision, recall, f_score, support = prf(gt, pred, average='macro')\n","    print(\"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(accuracy, precision, recall, f_score))\n","    return accuracy, precision, recall, f_score\n","\n","  def test(self):\n","    print(\"======================TEST MODE======================\")\n","    self.dagmm.eval()\n","\n","    ########################################################################\n","    ## train\n","    N = 0; mu_sum = 0; cov_sum = 0; gamma_sum = 0\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n","      \n","      batch_gamma_sum = torch.sum(gamma, dim=0)\n","      \n","      gamma_sum += batch_gamma_sum\n","      mu_sum += mu * batch_gamma_sum.unsqueeze(-1) # keep sums of the numerator only\n","      cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1) # keep sums of the numerator only\n","      \n","      N += input_data.size(0)\n","        \n","    train_phi = gamma_sum / N\n","    train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n","    train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n","    \n","    # print(\"N:\",N)\n","    # print(\"phi :\\n\",train_phi)\n","    # print(\"mu :\\n\",train_mu)\n","    # print(\"cov :\\n\",train_cov)\n","\n","    train_energy = []\n","    train_labels = []\n","    train_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      \n","      train_energy.append(sample_energy.data.cpu().numpy())\n","      train_z.append(z.data.cpu().numpy())\n","      # train_labels.append(labels.numpy())\n","\n","    train_energy = np.concatenate(train_energy,axis=0)\n","    train_z = np.concatenate(train_z,axis=0)\n","    # train_labels = np.concatenate(train_labels,axis=0)\n","    ########################################################################\n","    ## validation\n","    N = 0; mu_sum = 0; cov_sum = 0; gamma_sum = 0\n","\n","    for it, (input_data, labels) in enumerate(self.data_loader_v):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n","      \n","      batch_gamma_sum = torch.sum(gamma, dim=0)\n","      \n","      gamma_sum += batch_gamma_sum\n","      mu_sum += mu * batch_gamma_sum.unsqueeze(-1) # keep sums of the numerator only\n","      cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1) # keep sums of the numerator only\n","      \n","      N += input_data.size(0)\n","        \n","    val_phi = gamma_sum / N\n","    val_mu = mu_sum / gamma_sum.unsqueeze(-1)\n","    val_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n","    \n","    # print(\"N:\",N)\n","    # print(\"phi :\\n\",train_phi)\n","    # print(\"mu :\\n\",train_mu)\n","    # print(\"cov :\\n\",train_cov)\n","\n","    val_energy = []\n","    val_labels = []\n","    val_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=val_phi, mu=val_mu, cov=val_cov, size_average=False)\n","      \n","      val_energy.append(sample_energy.data.cpu().numpy())\n","      val_z.append(z.data.cpu().numpy())\n","      val_labels.append(labels.numpy())\n","\n","    val_energy = np.concatenate(val_energy,axis=0)\n","    val_z = np.concatenate(val_z,axis=0)\n","    val_labels = np.concatenate(val_labels,axis=0)\n","\n","    ########################################################################\n","    ## test\n","    test_energy = []; test_labels = []; test_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader_test):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, size_average=False)\n","      test_energy.append(sample_energy.data.cpu().numpy())\n","      test_z.append(z.data.cpu().numpy())\n","      # test_labels.append(labels.numpy())\n","\n","    test_energy = np.concatenate(test_energy,axis=0)\n","    test_z = np.concatenate(test_z,axis=0)\n","    # test_labels = np.concatenate(test_labels,axis=0)\n","\n","    combined_energy = np.concatenate([train_energy, val_energy, test_energy], axis=0)\n","    # combined_labels = np.concatenate([train_labels, val_labels, test_labels], axis=0)\n","\n","    thresh = np.percentile(combined_energy, 100 - 2)\n","    # print(\"Threshold :\", thresh)\n","\n","    pred = (test_energy > thresh).astype(int)\n","    # gt = test_labels.astype(int)\n","    print(\"unique:\", set(pred))\n","    from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n","    # accuracy = accuracy_score(gt,pred)\n","    # precision, recall, f_score, support = prf(gt, pred, average='binary')\n","    # print(\"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(accuracy, precision, recall, f_score))\n","    # return accuracy, precision, recall, f_score\n","    return pred, val_z, val_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2l-jtBmPbIS"},"outputs":[],"source":["class Cholesky(torch.autograd.Function):\n","    def forward(ctx, a):\n","      # l = torch.potrf(a, False)\n","      try:\n","        # l = torch.linalg.cholesky(a, False)\n","        l = torch.cholesky(a, False)\n","        ctx.save_for_backward(l)\n","      except:\n","        print(\"-------\")\n","        print(a)\n","        print(\"-------\")\n","      return l\n","    def backward(ctx, grad_output):\n","      l, = ctx.saved_variables\n","      linv = l.inverse()\n","      inner = torch.tril(torch.mm(l.t(), grad_output)) * torch.tril(\n","          1.0 - Variable(l.data.new(l.size(1)).fill_(0.51).diag()))\n","      s = torch.mm(linv.t(), torch.mm(inner, linv))\n","      return s\n","    \n","class DaGMM(nn.Module):\n","    \"\"\"Residual Block.\"\"\"\n","    def __init__(self, device, n_gmm = 2, latent_dim=3):\n","      super(DaGMM, self).__init__()\n","      self.device = device\n","      \n","      # layers = []\n","      # layers += [nn.Linear(30, 64)]\n","      # layers += [nn.BatchNorm1d(64)]\n","      # layers += [nn.LeakyReLU()]\n","      # layers += [nn.Linear(64,128)]\n","      # layers += [nn.BatchNorm1d(128)]\n","      # layers += [nn.LeakyReLU()]\n","      # self.encoder = nn.Sequential(*layers)\n","    \n","      # layers = []\n","      # layers += [nn.Linear(128, 64)]\n","      # layers += [nn.BatchNorm1d(64)]\n","      # layers += [nn.LeakyReLU()]\n","      # layers += [nn.Linear(64,30)]\n","      # self.decoder = nn.Sequential(*layers)\n","      \n","      layers = []\n","      layers += [nn.Linear(30,60)] #####\n","      layers += [nn.Tanh()]        \n","      layers += [nn.Linear(60,30)]\n","      layers += [nn.Tanh()]        \n","      layers += [nn.Linear(30,10)]\n","      layers += [nn.Tanh()]        \n","      layers += [nn.Linear(10,1)]\n","      self.encoder = nn.Sequential(*layers)\n","\n","      layers = []\n","      layers += [nn.Linear(1,10)]\n","      layers += [nn.Tanh()]        \n","      layers += [nn.Linear(10,30)]\n","      layers += [nn.Tanh()]        \n","      layers += [nn.Linear(30,60)]\n","      layers += [nn.Tanh()]        \n","      layers += [nn.Linear(60,30)]\n","      self.decoder = nn.Sequential(*layers)\n","\n","      layers = []\n","      layers += [nn.Linear(latent_dim,10)]\n","      layers += [nn.Tanh()]        \n","      layers += [nn.Dropout(p=0.5)]        \n","      layers += [nn.Linear(10,n_gmm)]\n","      layers += [nn.Softmax(dim=1)]\n","      self.estimation = nn.Sequential(*layers)\n","\n","      self.register_buffer(\"phi\", torch.zeros(n_gmm))\n","      self.register_buffer(\"mu\", torch.zeros(n_gmm,latent_dim))\n","      self.register_buffer(\"cov\", torch.zeros(n_gmm,latent_dim,latent_dim))\n","\n","    def relative_euclidean_distance(self, a, b):\n","      return (a-b).norm(2, dim=1) / a.norm(2, dim=1)\n","\n","    def forward(self, x):\n","      enc = self.encoder(x)\n","      dec = self.decoder(enc)\n","\n","      rec_cosine = F.cosine_similarity(x, dec, dim=1)\n","      rec_euclidean = self.relative_euclidean_distance(x, dec)\n","      z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n","\n","      print(\"======================= z ==================\\n\", z)\n","      print(\"======================= enc ==================\\n\", enc)\n","      print(\"======================= dec ==================\\n\", dec)\n","      print(\"======================= cosines ==================\\n\", rec_cosine)\n","      print(\"======================= euclidean ==================\\n\", rec_euclidean)\n","      print(\"xxxx\\n\", x, \"\\n\")\n","\n","      gamma = self.estimation(z)\n","      return enc, dec, z, gamma\n","\n","    def compute_gmm_params(self, z, gamma):\n","      N = gamma.size(0)\n","      # K\n","      sum_gamma = torch.sum(gamma, dim=0)\n","\n","      # K\n","      phi = (sum_gamma / N)\n","      self.phi = phi.data\n","\n","      # K x D\n","      mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n","      self.mu = mu.data\n","      # z = N x D\n","      # mu = K x D\n","      # gamma N x K\n","\n","      # z_mu = N x K x D\n","      z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n","\n","      # z_mu_outer = N x K x D x D\n","      z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n","\n","      # K x D x D\n","      cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim = 0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n","      self.cov = cov.data\n","      return phi, mu, cov\n","        \n","    def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n","      if phi is None : phi = self.phi #self.to_var(self.phi)\n","      if mu is None : mu = self.mu # self.to_var(self.mu)\n","      if cov is None : cov = self.cov #self.to_var(self.cov)\n","\n","      k, D, _ = cov.size()\n","      z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n","\n","      cov_inverse = []\n","      det_cov = []\n","      cov_diag = 0\n","      eps = 1e-12\n","      for i in range(k):\n","        # K x D x D\n","        # cov_k = cov[i] + self.to_var(torch.eye(D)*eps)\n","        cov_k = cov[i] + (torch.eye(D)*eps).to(device = self.device)\n","        cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n","        \n","        if np.isnan((cov_k.cpu() * (2*np.pi))[0,0].item()) : \n","          print(cov_k)\n","          import time; time.sleep(30)\n","          continue\n","        #det_cov.append(np.linalg.det(cov_k.data.cpu().numpy()* (2*np.pi)))\n","        det_cov.append((Cholesky.apply(cov_k.cpu() * (2*np.pi)).diag().prod()).unsqueeze(0))\n","        cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n","\n","      # K x D x D\n","      cov_inverse = torch.cat(cov_inverse, dim=0)\n","      # K\n","      det_cov = torch.cat(det_cov).cuda()\n","      #det_cov = to_var(torch.from_numpy(np.float32(np.array(det_cov))))\n","\n","      # N x K\n","      exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n","      # for stability (logsumexp)\n","      max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n","      exp_term = torch.exp(exp_term_tmp - max_val)\n","\n","      # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (det_cov).unsqueeze(0), dim = 1) + eps)\n","      sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt(det_cov)).unsqueeze(0), dim = 1) + eps)\n","      # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt((2*np.pi)**D * det_cov)).unsqueeze(0), dim = 1) + eps)\n","\n","      if size_average : sample_energy = torch.mean(sample_energy)\n","      return sample_energy, cov_diag\n","\n","    def to_var(x, volatile=False):\n","        if torch.cuda.is_available() : x = x.cuda()\n","        print(\"=====\", x)\n","        import time\n","        # time.sleep(30)\n","        return Variable(x, volatile=volatile)\n","\n","    def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n","        recon_error = torch.mean((x - x_hat) ** 2)\n","        phi, mu, cov = self.compute_gmm_params(z, gamma)\n","        sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n","        loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n","        return loss, sample_energy, recon_error, cov_diag\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"191fXvw8KIsyAIFzcvtwAtAQpT0neZLDH"},"id":"sDr1-i0dQUPl","outputId":"56dbebd1-439b-49ed-a5a2-7c84df88817d"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import os\n","import argparse\n","# from solver import Solver\n","# from data_loader import get_loader\n","from torch.backends import cudnn\n","# from utils import *\n","\n","def str2bool(v):\n","    return v.lower() in ('true')\n","\n","def main(config):\n","    # For fast training\n","    cudnn.benchmark = True\n","\n","    # Create directories if not exist\n","    # mkdir(config.log_path)\n","    # mkdir(config.model_save_path)\n","    # wandb.init()\n","    data_loader, data_loader_v, data_loader_test = get_loader(config.data_path, batch_size=config.batch_size, mode=config.mode)\n","    \n","    import torch\n","    USE_CUDA = torch.cuda.is_available()\n","    device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n","\n","    # Solver\n","    solver = Solver(device, data_loader, data_loader_v, data_loader_test, vars(config))\n","\n","    if config.mode == 'train' : \n","      solver.train()\n","      return solver\n","    elif config.mode == 'test' : \n","      pred, val_z, val_labels = solver.test()\n","      return solver, pred, val_z, val_labels\n","\n","if __name__ == '__main__':\n","    import easydict\n","    args = easydict.EasyDict({\n","        \"lr\" : 1e-4,\n","        \"num_epochs\" : 100,\n","        \"batch_size\" : 512,\n","        \"gmm_k\" : 4,\n","        \"lambda_energy\" : 0.1,\n","        \"lambda_cov_diag\" : 0.005,\n","        \"pretrained_model\" : '',\n","        # \"pretrained_model\" : None,\n","        \"mode\" : 'train',\n","        # \"mode\" : \"test\",\n","        \"use_tensorboard\" : False,\n","        \"data_path\" : \"./\",\n","        \"log_path\" : './logs',\n","        \"model_save_path\" : './models',\n","        \"log_step\" : 100,\n","        \"sample_step\" : 194,\n","        \"model_save_step\" : 194\n","    })\n","    config = args\n","    # args = vars(config)\n","    print('------------ Options -------------')\n","    for k, v in sorted(args.items()):\n","        print('%s: %s' % (str(k), str(v)))\n","    print('-------------- End ----------------')\n","\n","    solver, pred, val_z, val_labels = main(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRB3q6iOToQH"},"outputs":[],"source":["pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QyghdfdApsg6"},"outputs":[],"source":["len(pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUwHLTVYqHit"},"outputs":[],"source":["submit = pd.read_csv('./sample_submission.csv')\n","submit['Class'] = pred\n","submit.to_csv('./submit_0.01.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaaeY3fQrUXq"},"outputs":[],"source":["from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.pyplot as plt\n","%matplotlib notebook\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","# ax.scatter(test_z[:,1],test_z[:,0], test_z[:,2], c=test_labels.astype(int))\n","ax.scatter(val_z[:,1],val_z[:,0], val_z[:,2], c=val_labels.astype(int), alpha=0.1)\n","\n","ax.set_xlabel('Encoded')\n","ax.set_ylabel('Euclidean')\n","ax.set_zlabel('Cosine')\n","plt.show()\n","plt.savefig('fig2.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0Ehzc9SC3uo"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wj3ZW24iKP3P"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8TCy533GKXZO"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"11l-EZ3jh4m35otW_xCiLfxOGbFPrhGwe","authorship_tag":"ABX9TyPiuVNfX0P6PG0ZeCLjAGl0"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}