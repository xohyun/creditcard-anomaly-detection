{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1GtDfKW90iDn_irx9Ij7ckkNpI2VtLWvQ","authorship_tag":"ABX9TyNPJVtFFdWl22yTGCBRWhY0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","# drive.mount('/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data')\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/IITP/sohyun/creditcard_prediction/data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aoTfHzuB_M7u","executionInfo":{"status":"ok","timestamp":1670997907587,"user_tz":-540,"elapsed":11904,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"9a34d914-19c6-4198-86d5-3c9b1857048c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n"]}]},{"cell_type":"code","source":["!pip install wandb -qqq\n","import wandb\n","wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"pyCrbam-X2js","executionInfo":{"status":"ok","timestamp":1670997966335,"user_tz":-540,"elapsed":54547,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"896f5975-3095-41a3-b75a-9dde04f9bf38"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 1.9 MB 4.9 MB/s \n","\u001b[K     |████████████████████████████████| 168 kB 65.6 MB/s \n","\u001b[K     |████████████████████████████████| 182 kB 76.8 MB/s \n","\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n","\u001b[K     |████████████████████████████████| 168 kB 74.4 MB/s \n","\u001b[K     |████████████████████████████████| 166 kB 76.2 MB/s \n","\u001b[K     |████████████████████████████████| 166 kB 61.4 MB/s \n","\u001b[K     |████████████████████████████████| 162 kB 71.1 MB/s \n","\u001b[K     |████████████████████████████████| 162 kB 81.4 MB/s \n","\u001b[K     |████████████████████████████████| 158 kB 87.0 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 75.5 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 86.9 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 80.2 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 88.1 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 88.2 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 85.5 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 88.8 MB/s \n","\u001b[K     |████████████████████████████████| 156 kB 85.1 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "]},{"name":"stdout","output_type":"stream","text":["··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gPauNGcl-kic","executionInfo":{"status":"ok","timestamp":1670997974613,"user_tz":-540,"elapsed":5171,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"outputs":[],"source":["import random\n","import pandas as pd\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","# wandb.init(project=\"\") # wandb init\n","\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    def __init__(self, df, eval_mode):\n","        self.df = df\n","        self.eval_mode = eval_mode\n","        if self.eval_mode:\n","            self.labels = self.df['Class'].values\n","            self.df = self.df.drop(columns=['Class']).values\n","        else:\n","            self.df = self.df.values\n","        \n","    def __getitem__(self, index):\n","        if self.eval_mode:\n","            self.x = self.df[index]\n","            self.y = self.labels[index]\n","            return torch.Tensor(self.x), self.y\n","        else:\n","            self.x = self.df[index]\n","            return torch.Tensor(self.x)\n","        \n","    def __len__(self):\n","        return len(self.df)\n","\n","class AutoEncoder(nn.Module):\n","    def __init__(self):\n","        super(AutoEncoder, self).__init__()\n","        self.Encoder = nn.Sequential(\n","            nn.Linear(30,64),\n","            nn.BatchNorm1d(64),\n","            nn.LeakyReLU(),\n","            nn.Linear(64,128),\n","            nn.BatchNorm1d(128),\n","            nn.LeakyReLU(),\n","        )\n","        self.Decoder = nn.Sequential(\n","            nn.Linear(128,64),\n","            nn.BatchNorm1d(64),\n","            nn.LeakyReLU(),\n","            nn.Linear(64,30),\n","        )\n","        \n","    def forward(self, x):\n","        x = self.Encoder(x)\n","        x = self.Decoder(x)\n","        return x\n","\n","class Trainer():\n","    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.scheduler = scheduler\n","        self.device = device\n","        # Loss Function\n","        self.criterion = nn.L1Loss().to(self.device)\n","        \n","    def fit(self, config, modelNum=None):\n","        self.model.to(self.device)\n","        best_score = 0\n","        for epoch in range(config.EPOCHS):\n","            self.model.train()\n","            train_loss = []\n","            for x in iter(self.train_loader):\n","                x = x.float().to(self.device)\n","                self.optimizer.zero_grad()\n","\n","                _x = self.model(x)\n","                loss = self.criterion(x, _x)\n","\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                train_loss.append(loss.item())\n","\n","            score = self.validation(self.model, config.thr)\n","            # print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n","            \n","            wandb.log({\n","                \"validation f1\": score,\n","                \"loss\": loss\n","            })\n","            if self.scheduler is not None:\n","              self.scheduler.step(score)\n","\n","            if best_score < score:\n","              print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n","              best_score = score\n","              if modelNum :\n","                torch.save(self.model.module.state_dict(), f'./best_model{modelNum}.pth', _use_new_zipfile_serialization=False)\n","              else :\n","                torch.save(self.model.module.state_dict(), f'./best_model.pth', _use_new_zipfile_serialization=False)\n","  \n","    def validation(self, eval_model, thr):\n","        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n","        eval_model.eval()\n","        pred = []\n","        true = []\n","        with torch.no_grad():\n","            for x, y in iter(self.val_loader):\n","                x = x.float().to(self.device)\n","\n","                _x = self.model(x)\n","                diff = cos(x, _x).cpu().tolist()\n","                batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n","                pred += batch_pred\n","                true += y.tolist()\n","\n","        return f1_score(true, pred, average='macro')"],"metadata":{"id":"kijk5u73jo-B","executionInfo":{"status":"ok","timestamp":1670997977855,"user_tz":-540,"elapsed":290,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def prediction(model, thr, test_loader, device):\n","  model.to(device)\n","  model.eval()\n","  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n","  pred = []\n","  with torch.no_grad():\n","    for x in iter(test_loader):\n","      x = x.float().to(device)\n","      \n","      _x = model(x)\n","      \n","      diff = cos(x, _x).cpu().tolist()\n","      batch_pred = np.where(np.array(diff)<thr, 1, 0).tolist()\n","      pred += batch_pred\n","  return pred"],"metadata":{"id":"z2KhkzS2QK_r","executionInfo":{"status":"ok","timestamp":1670997980729,"user_tz":-540,"elapsed":492,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def main(config):\n","  seed_everything(config.SEED) # Seed fix\n","\n","  #---# DATA #---#\n","  train_df = pd.read_csv('./train.csv')\n","  train_df = train_df.drop(columns=['ID'])\n","  val_df = pd.read_csv('./val.csv')\n","  val_df = val_df.drop(columns=['ID'])\n","  test_df = pd.read_csv('./test.csv')\n","  test_df = test_df.drop(columns=['ID'])\n","  models = [] # list of models\n","\n","  val_dataset = MyDataset(df = val_df, eval_mode=True)\n","  val_loader = DataLoader(val_dataset, batch_size=config.BS, shuffle=False)\n","  test_dataset = MyDataset(test_df, False)\n","  test_loader = DataLoader(test_dataset, batch_size=config.BS, shuffle=False, num_workers=6)\n","\n","  # for refine\n","  train_dataset = MyDataset(df=train_df, eval_mode=False)\n","  train_loader = DataLoader(train_dataset, batch_size=config.BS, shuffle=True)\n","  model = nn.DataParallel(AutoEncoder())\n","  model.eval()\n","  optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-2)\n","  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, threshold_mode='abs', min_lr=1e-8, verbose=True)\n","  \n","  trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n","  trainer.fit(config)\n","\n","  model = AutoEncoder()\n","  model.load_state_dict(torch.load(f'./best_model.pth'))\n","  model = nn.DataParallel(model)\n","  model.eval()\n","  preds = prediction(model, config.thr, train_loader, device)\n","  print(\"<<<없애는 anomay 수>>> \", sum(preds))\n","  train_df_pseudo = train_df\n","  train_df_pseudo['Class'] = preds\n","  \n","  idx_anomal = train_df_pseudo[train_df_pseudo['Class'] == 1].index\n","  train_df_pseudo = train_df_pseudo.drop(idx_anomal)\n","  train_df_pseudo = train_df_pseudo.drop(columns=['Class'])\n","  train_df_pseudo = train_df_pseudo.reset_index(drop=True)\n","\n","  # for ensemble\n","  for i in range(config.K):\n","    choose_idx = np.random.choice(train_df_pseudo.shape[0], 50000, replace=True)\n","    train_df_choose = train_df_pseudo.loc[choose_idx,:]\n","    train_dataset = MyDataset(df=train_df_choose, eval_mode=False)\n","    train_loader = DataLoader(train_dataset, batch_size=config.BS, shuffle=True)\n","\n","    model = nn.DataParallel(AutoEncoder())\n","    model.eval()\n","    optimizer = torch.optim.Adam(params = model.parameters(), lr = config.LR)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, threshold_mode='abs', min_lr=1e-8, verbose=True)\n","\n","    trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n","    trainer.fit(config, modelNum=(i+1))\n","\n","    models.append(model)\n","\n","  # # for test\n","  # model_preds = []\n","  # for i in range(config.K):\n","  #   model = AutoEncoder()\n","  #   model.load_state_dict(torch.load(f'./best_model{(i+1)}.pth'))\n","  #   model = nn.DataParallel(model)\n","  #   model.eval()\n","  #   preds = prediction(model, 0.97, test_loader, device)\n","  #   model_preds.append(preds)\n","\n","  # model_pred_df = pd.DataFrame(model_preds).transpose()\n","  # row_sum = model_pred_df.sum(axis=1)\n","  # pred = np.where(row_sum > 3, 1, 0) # 클수록 anomaly\n","  \n","  # return pred\n","\n","if __name__ == '__main__':\n","  wandb.init()\n","  import easydict\n","  args = easydict.EasyDict({\n","      \"K\" : 10,\n","      \"EPOCHS\" : 200, #65 ## 400\n","      \"LR\" : 1e-2,\n","      \"BS\" : 16384, #16384\n","      \"SEED\" : 1004,\n","      \"thr\" : 0.95\n","  })\n","  config = args\n","\n","  print('------------ Options -------------')\n","  for k, v in sorted(args.items()):\n","    print('%s: %s' % (str(k), str(v)))\n","  print('-------------- End ----------------')\n","\n","  pred = main(config)"],"metadata":{"id":"MOLXoEwK8TqY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import easydict\n","args = easydict.EasyDict({\n","    \"K\" : 10,\n","    \"EPOCHS\" : 200, #65 ## 400\n","    \"LR\" : 1e-2,\n","    \"BS\" : 16384, #16384\n","    \"SEED\" : 1004,\n","    \"thr\" : 0.95\n","})\n","config = args\n","\n","#------------------#\n","#---# For test #---#\n","#------------------#\n","test_df = pd.read_csv('./test.csv')\n","test_df = test_df.drop(columns=['ID'])\n","test_dataset = MyDataset(test_df, False)\n","test_loader = DataLoader(test_dataset, batch_size=config.BS, shuffle=False, num_workers=6)\n","\n","model_preds = []\n","for i in range(config.K):\n","  model = AutoEncoder()\n","  model.load_state_dict(torch.load(f'./best_model{(i+1)}.pth'))\n","  model = nn.DataParallel(model)\n","  model.eval()\n","  preds = prediction(model, config.thr, test_loader, device)\n","  model_preds.append(preds)\n","\n","model_pred_df = pd.DataFrame(model_preds).transpose()\n","row_sum = model_pred_df.sum(axis=1)\n","pred = np.where(row_sum > 7, 1, 0) # 클수록 anomaly"],"metadata":{"id":"Y5lHDpl11Olc","executionInfo":{"status":"ok","timestamp":1659577007668,"user_tz":-540,"elapsed":11948,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba80fc18-7503-4e52-c942-c3f76b4d1ba5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["#---# For submission #---#\n","submit = pd.read_csv('./sample_submission.csv')\n","submit['Class'] = pred\n","submit.to_csv('./submit_autoencoder_with_vote_clean_10000.csv', index=False)"],"metadata":{"id":"IgdZWXD0IJkR"},"execution_count":null,"outputs":[]}]}