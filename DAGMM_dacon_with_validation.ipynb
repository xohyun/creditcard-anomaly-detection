{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DAGMM_dacon_with_validation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMDZb0H6NZfiH44Wo4/lV+i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ik9sLgGJ7G59","executionInfo":{"status":"ok","timestamp":1659661746452,"user_tz":-540,"elapsed":1912,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"4ea53c33-2c7f-4690-b73a-630aa697ff6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/MyDrive/IITP/sohyun/creditcard_prediction/data"]},{"cell_type":"code","source":["!pip install wandb -qqq\n","import wandb\n","wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P3CNrkhz2jjy","executionInfo":{"status":"ok","timestamp":1659661763349,"user_tz":-540,"elapsed":11181,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"f7017555-919e-4c80-fb78-7aa59609f3fe"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msohyun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import torch\n","import os\n","import random\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from PIL import Image\n","import h5py\n","import numpy as np\n","import collections\n","import numbers\n","import math\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import numpy as np\n","import os\n","import argparse\n","from torch.backends import cudnn\n","from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n","\n","THR = 0.1\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","np.random.seed(0)\n","cudnn.benchmark = False\n","cudnn.deterministic = True\n","random.seed(0)\n","# torch.use_deterministic_algorithms(True)\n","\n","def to_var(x, volatile=False):\n","  if torch.cuda.is_available():\n","    x = x.cuda()\n","  return Variable(x, volatile=volatile)\n","\n","def mkdir(directory):\n","  if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","class KDD99Loader(object):\n","  def __init__(self, data_path, mode=\"train\"):\n","    self.mode=mode\n","\n","    ## 이부분 원래 윗셀\n","    train_df = pd.read_csv('./train.csv')\n","    train_df = train_df.drop(columns=['ID'])\n","    val_df = pd.read_csv('./val.csv')\n","    val_df = val_df.drop(columns=['ID'])\n","    test_df = pd.read_csv('./test.csv')\n","    test_df = test_df.drop(columns=['ID'])\n","\n","    #-------------------#\n","    #---# Normalize #---#\n","    #-------------------#\n","    # case 1 - standardscaler\n","    scaler_n = StandardScaler()\n","    scaler_n.fit(train_df)\n","\n","    val_x = val_df.drop(columns=['Class'])\n","    train_x_scaleN = pd.DataFrame(scaler_n.transform(train_df), columns = train_df.columns) # 확인 : train_x_scaleN.mean(), train_x_scaleN.var()\n","    val_x_scaleN = pd.DataFrame(scaler_n.transform(val_x), columns = val_x.columns)\n","    test_x_scaleN = pd.DataFrame(scaler_n.transform(test_df), columns = test_df.columns)\n","\n","    train_df = train_x_scaleN\n","    val_df = pd.concat([val_x_scaleN, pd.DataFrame(val_df['Class'])], axis=1)\n","    test_df = test_x_scaleN\n","    ##\n","\n","    self.train = train_df.values\n","    self.train_labels = np.tile(0, len(train_df))\n","\n","    self.valid = val_df.drop(columns = ['Class']).values\n","    self.valid_labels = val_df['Class'].values\n","\n","    self.test = test_df.values\n","    self.test_labels = np.tile(0, len(test_df))\n","    # self.test = val_df.drop(columns = ['Class']).values\n","    # self.test_labels = val_df['Class'].values\n","\n","  def __len__(self):\n","    \"\"\"\n","    Number of images in the object dataset.\n","    \"\"\"\n","    if self.mode == \"train\":\n","      return self.train.shape[0]\n","    elif self.mode == \"valid\":\n","      return self.valid.shape[0]\n","    else:\n","      return self.test.shape[0]\n","\n","  def __getitem__(self, index):\n","    if self.mode == \"train\":\n","      return np.float32(self.train[index]), np.float32(self.train_labels[index])\n","    elif self.mode == \"valid\":\n","      return np.float32(self.valid[index]), np.float32(self.valid_labels[index])\n","    else:\n","      return np.float32(self.test[index]), np.float32(self.test_labels[index])\n","      \n","\n","def get_loader(data_path, batch_size, mode='train'):\n","  \"\"\"Build and return data loader.\"\"\"\n","  dataset = KDD99Loader(data_path, mode)\n","\n","  shuffle = False\n","  if mode == 'train': shuffle = True\n","\n","  data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n","  return data_loader"],"metadata":{"id":"uSLkzBK08gZE","executionInfo":{"status":"ok","timestamp":1659665352305,"user_tz":-540,"elapsed":368,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import torchvision\n","from torch.autograd import Variable\n","import itertools\n","\n","class Cholesky(torch.autograd.Function):\n","  def forward(ctx, a):\n","    l = torch.cholesky(a, False)\n","    ctx.save_for_backward(l)\n","    return l\n","  def backward(ctx, grad_output):\n","    # l, = ctx.saved_variables\n","    l, = ctx.saved_tensors\n","    linv = l.inverse()\n","    inner = torch.tril(torch.mm(l.t(), grad_output)) * torch.tril(\n","        1.0 - Variable(l.data.new(l.size(1)).fill_(0.5).diag()))\n","    s = torch.mm(linv.t(), torch.mm(inner, linv))\n","    return s\n","  \n","class DaGMM(nn.Module):\n","  \"\"\"Residual Block.\"\"\"\n","  def __init__(self, n_gmm = 4, latent_dim=12):\n","    super(DaGMM, self).__init__()\n","\n","    layers = []\n","    # layers += [nn.Linear(118,60)]\n","    # layers += [nn.Tanh()]        \n","    # layers += [nn.Linear(60,30)]\n","    # layers += [nn.Tanh()]        \n","    layers += [nn.Linear(30,25)]\n","    layers += [nn.Tanh()]         \n","    layers += [nn.Linear(25,20)]\n","    layers += [nn.Tanh()]         \n","    layers += [nn.Linear(20,10)]\n","    # layers += [nn.Tanh()]         \n","    # layers += [nn.Linear(15,10)]\n","\n","    self.encoder = nn.Sequential(*layers)\n","\n","    layers = []\n","    # layers += [nn.Linear(10,15)]\n","    # layers += [nn.Tanh()]        \n","    layers += [nn.Linear(10,20)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Linear(20,25)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Linear(25,30)]\n","    # layers += [nn.Tanh()]        \n","    # layers += [nn.Linear(60,118)]\n","\n","    self.decoder = nn.Sequential(*layers)\n","\n","    layers = []\n","    layers += [nn.Linear(latent_dim,10)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Dropout(p=0.5)]        \n","    layers += [nn.Linear(10,n_gmm)]\n","    layers += [nn.Softmax(dim=1)]\n","\n","\n","    self.estimation = nn.Sequential(*layers)\n","\n","    self.register_buffer(\"phi\", torch.zeros(n_gmm))\n","    self.register_buffer(\"mu\", torch.zeros(n_gmm,latent_dim))\n","    self.register_buffer(\"cov\", torch.zeros(n_gmm,latent_dim,latent_dim))\n","\n","  def relative_euclidean_distance(self, a, b):\n","    return (a-b).norm(2, dim=1) / a.norm(2, dim=1)\n","\n","  def forward(self, x):\n","    enc = self.encoder(x)\n","    dec = self.decoder(enc)\n","    rec_cosine = F.cosine_similarity(x, dec, dim=1)\n","    rec_euclidean = self.relative_euclidean_distance(x, dec)\n","    z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n","    gamma = self.estimation(z)\n","    return enc, dec, z, gamma\n","\n","  def compute_gmm_params(self, z, gamma):\n","    N = gamma.size(0)\n","    # K\n","    sum_gamma = torch.sum(gamma, dim=0)\n","\n","    # K\n","    phi = (sum_gamma / N)\n","    self.phi = phi.data\n","\n","    # K x D\n","    mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n","    self.mu = mu.data\n","    # z = N x D\n","    # mu = K x D\n","    # gamma N x K\n","\n","    # z_mu = N x K x D\n","    z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n","\n","    # z_mu_outer = N x K x D x D\n","    z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n","\n","    # K x D x D\n","    cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim = 0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n","    self.cov = cov.data\n","\n","    return phi, mu, cov\n","      \n","  def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n","    if phi is None: phi = to_var(self.phi)\n","    if mu is None: mu = to_var(self.mu)\n","    if cov is None: cov = to_var(self.cov)\n","\n","    k, D, _ = cov.size()\n","\n","    z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n","\n","    cov_inverse = []\n","    det_cov = []\n","    cov_diag = 0\n","    eps = 1e-12\n","    for i in range(k):\n","      # K x D x D\n","      cov_k = cov[i] + to_var(torch.eye(D)*eps)\n","      cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n","\n","      #det_cov.append(np.linalg.det(cov_k.data.cpu().numpy()* (2*np.pi)))\n","      det_cov.append((Cholesky.apply(cov_k.cpu() * (2*np.pi)).diag().prod()).unsqueeze(0))\n","      cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n","\n","    # K x D x D\n","    cov_inverse = torch.cat(cov_inverse, dim=0)\n","    # K\n","    det_cov = torch.cat(det_cov).cuda()\n","    #det_cov = to_var(torch.from_numpy(np.float32(np.array(det_cov))))\n","\n","    # N x K\n","    exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n","    # for stability (logsumexp)\n","    max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n","\n","    exp_term = torch.exp(exp_term_tmp - max_val)\n","\n","    # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (det_cov).unsqueeze(0), dim = 1) + eps)\n","    sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt(det_cov)).unsqueeze(0), dim = 1) + eps)\n","    # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt((2*np.pi)**D * det_cov)).unsqueeze(0), dim = 1) + eps)\n","\n","\n","    if size_average:\n","      sample_energy = torch.mean(sample_energy)\n","\n","    return sample_energy, cov_diag\n","\n","\n","  def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n","    recon_error = torch.mean((x - x_hat) ** 2)\n","    phi, mu, cov = self.compute_gmm_params(z, gamma)\n","    sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n","    loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n","    return loss, sample_energy, recon_error, cov_diag"],"metadata":{"id":"MplRhuqf8CBe","executionInfo":{"status":"ok","timestamp":1659665354216,"user_tz":-540,"elapsed":387,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import os\n","import time\n","import datetime\n","from torch.autograd import grad\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","import IPython\n","from tqdm import tqdm\n","\n","class Solver(object):\n","  DEFAULTS = {}   \n","  def __init__(self, data_loader, config):\n","    # Data loader\n","    self.__dict__.update(Solver.DEFAULTS, **config)\n","    self.data_loader = data_loader\n","\n","    # Build tensorboard if use\n","    self.build_model()\n","    # if self.use_tensorboard:\n","    #     self.build_tensorboard()\n","\n","    # Start with trained model\n","    if self.pretrained_model:\n","      self.load_pretrained_model()\n","\n","  def build_model(self):\n","    # Define model\n","    self.dagmm = DaGMM(self.gmm_k)\n","\n","    # Optimizers\n","    self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr)\n","\n","    # Print networks\n","    self.print_network(self.dagmm, 'DaGMM')\n","\n","    if torch.cuda.is_available():\n","      self.dagmm.cuda()\n","\n","  def print_network(self, model, name):\n","    num_params = 0\n","    for p in model.parameters():\n","      num_params += p.numel()\n","    print(name)\n","    print(model)\n","    print(\"The number of parameters: {}\".format(num_params))\n","\n","  def load_pretrained_model(self):\n","    self.dagmm.load_state_dict(torch.load(os.path.join(\n","      self.model_save_path, '{}_dagmm.pth'.format(self.pretrained_model))))\n","\n","    # print(\"phi\", self.dagmm.phi,\"mu\",self.dagmm.mu, \"cov\",self.dagmm.cov)\n","\n","    print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n","\n","  # def build_tensorboard(self):\n","  #     from logger import Logger\n","  #     self.logger = Logger(self.log_path)\n","\n","  def reset_grad(self):\n","    self.dagmm.zero_grad()\n","\n","  def to_var(self, x, volatile=False):\n","    if torch.cuda.is_available():\n","        x = x.cuda()\n","    return Variable(x, volatile=volatile)\n","\n","  def train(self):\n","      iters_per_epoch = len(self.data_loader)\n","\n","      # Start with trained model if exists\n","      if self.pretrained_model:\n","          start = int(self.pretrained_model.split('_')[0])\n","      else:\n","          start = 0\n","\n","      # Start training\n","      iter_ctr = 0\n","      start_time = time.time()\n","    \n","      self.ap_global_train = np.array([0,0,0])\n","      best_f_score = -1\n","      for e in range(start, self.num_epochs):\n","        sum_total_loss = 0; sum_sample_energy = 0; sum_recon_error = 0; sum_cov_diag = 0 \n","        for i, (input_data, labels) in enumerate(tqdm(self.data_loader)):\n","          iter_ctr += 1\n","          start = time.time()\n","\n","          input_data = self.to_var(input_data)\n","\n","          total_loss,sample_energy, recon_error, cov_diag = self.dagmm_step(input_data)\n","          # Logging\n","          loss = {}\n","          loss['total_loss'] = total_loss.data.item()\n","          loss['sample_energy'] = sample_energy.item()\n","          loss['recon_error'] = recon_error.item()\n","          loss['cov_diag'] = cov_diag.item()\n","\n","          sum_total_loss += total_loss.data.item()\n","          sum_sample_energy += sample_energy.item()\n","          sum_recon_error += recon_error.item()\n","          sum_cov_diag += cov_diag.item()\n","\n","\n","          # Print out log info\n","          # if (i+1) % self.log_step == 0:\n","          #     elapsed = time.time() - start_time\n","          #     total_time = ((self.num_epochs*iters_per_epoch)-(e*iters_per_epoch+i)) * elapsed/(e*iters_per_epoch+i+1)\n","          #     epoch_time = (iters_per_epoch-i)* elapsed/(e*iters_per_epoch+i+1)\n","              \n","          #     epoch_time = str(datetime.timedelta(seconds=epoch_time))\n","          #     total_time = str(datetime.timedelta(seconds=total_time))\n","          #     elapsed = str(datetime.timedelta(seconds=elapsed))\n","\n","          #     lr_tmp = []\n","          #     for param_group in self.optimizer.param_groups:\n","          #         lr_tmp.append(param_group['lr'])\n","          #     tmplr = np.squeeze(np.array(lr_tmp))\n","\n","          #     log = \"Elapsed {}/{} -- {} , Epoch [{}/{}], Iter [{}/{}], lr {}\".format(\n","          #         elapsed,epoch_time,total_time, e+1, self.num_epochs, i+1, iters_per_epoch, tmplr)\n","\n","          #     for tag, value in loss.items():\n","          #         log += \", {}: {:.4f}\".format(tag, value)\n","\n","          #     IPython.display.clear_output()\n","          #     print(log)\n","\n","              # if self.use_tensorboard:\n","              #     for tag, value in loss.items()|:\n","              #         self.logger.scalar_summary(tag, value, e * iters_per_epoch + i + 1)\n","              # else:\n","              #     plt_ctr = 1\n","              #     if not hasattr(self,\"loss_logs\"):\n","              #         self.loss_logs = {}\n","              #         for loss_key in loss:\n","              #             self.loss_logs[loss_key] = [loss[loss_key]]\n","              #             plt.subplot(2,2,plt_ctr)\n","              #             plt.plot(np.array(self.loss_logs[loss_key]), label=loss_key)\n","              #             plt.legend()\n","              #             plt_ctr += 1\n","              #     else:\n","              #         for loss_key in loss:\n","              #             self.loss_logs[loss_key].append(loss[loss_key])\n","              #             plt.subplot(2,2,plt_ctr)\n","              #             plt.plot(np.array(self.loss_logs[loss_key]), label=loss_key)\n","              #             plt.legend()\n","              #             plt_ctr += 1\n","\n","              #     plt.show()\n","\n","          # print(\"phi\", self.dagmm.phi,\"mu\",self.dagmm.mu, \"cov\",self.dagmm.cov)\n","          \n","          # Save model checkpoints      \n","          # if (i+1) % self.model_save_step == 0:\n","          #   torch.save(self.dagmm.state_dict(), os.path.join(self.model_save_path, '{}_{}_dagmm.pth'.format(e+1, i+1)))\n","        accuracy, precision, recall, f_score = self.validation()      \n","        if f_score > best_f_score :\n","          torch.save(self.dagmm.state_dict(), os.path.join(self.model_save_path, '{}_{}_dagmm.pth'.format(e+1, i+1)))\n","        self.data_loader.dataset.mode=\"train\" # 다시 바꿔주기\n","\n","          # print(\"total_e\", sum_total_loss)\n","          # print(\"\", sum_sample_energy)\n","          # print(\"re\", sum_recon_error)\n","          # print(\"cov\", sum_cov_diag)\n","          # print(\"--\")\n","        wandb.log({\n","            \"total loss\": sum_total_loss,\n","            \"sample energy\": sum_sample_energy,\n","            \"recon error\": sum_recon_error,\n","            \"cov\": sum_cov_diag,\n","            \"accuracy\": accuracy,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"f_score\": f_score\n","        })\n","\n","  def dagmm_step(self, input_data):\n","    self.dagmm.train()\n","    enc, dec, z, gamma = self.dagmm(input_data)\n","\n","    total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma, self.lambda_energy, self.lambda_cov_diag)\n","\n","    self.reset_grad()\n","    total_loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(self.dagmm.parameters(), 5)\n","    self.optimizer.step()\n","\n","    return total_loss,sample_energy, recon_error, cov_diag\n","\n","  def validation(self):\n","    self.dagmm.eval()\n","    self.data_loader.dataset.mode=\"train\"\n","\n","    N = 0; mu_sum = 0; cov_sum = 0; gamma_sum = 0\n","\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n","      \n","      batch_gamma_sum = torch.sum(gamma, dim=0)\n","      \n","      gamma_sum += batch_gamma_sum\n","      mu_sum += mu * batch_gamma_sum.unsqueeze(-1) # keep sums of the numerator only\n","      cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1) # keep sums of the numerator only\n","      \n","      N += input_data.size(0)\n","      \n","    train_phi = gamma_sum / N\n","    train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n","    train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n","\n","    # print(\"N:\",N)\n","    # print(\"phi :\\n\",train_phi)\n","    # print(\"mu :\\n\",train_mu)\n","    # print(\"cov :\\n\",train_cov)\n","\n","    train_energy = []\n","    train_labels = []\n","    train_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      # sample_energy, cov_diag = self.dagmm.compute_energy(z, size_average=False)\n","\n","      train_energy.append(sample_energy.data.cpu().numpy())\n","      train_z.append(z.data.cpu().numpy())\n","      train_labels.append(labels.numpy())\n","\n","    train_energy = np.concatenate(train_energy,axis=0)\n","    train_z = np.concatenate(train_z,axis=0)\n","    train_labels = np.concatenate(train_labels,axis=0)\n","\n","    ###### use validation data\n","    self.data_loader.dataset.mode=\"valid\"\n","    valid_energy = []\n","    valid_labels = []\n","    valid_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      valid_energy.append(sample_energy.data.cpu().numpy())\n","      valid_z.append(z.data.cpu().numpy())\n","      valid_labels.append(labels.numpy())\n","\n","    valid_energy = np.concatenate(valid_energy,axis=0)\n","    valid_z = np.concatenate(valid_z,axis=0)\n","    valid_labels = np.concatenate(valid_labels,axis=0)\n","\n","    combined_energy = np.concatenate([train_energy, valid_energy], axis=0)\n","    combined_labels = np.concatenate([train_labels, valid_labels], axis=0)\n","\n","\n","    thresh = np.percentile(combined_energy, 100 - THR)\n","    pred = (valid_energy > thresh).astype(int)\n","    gt = valid_labels.astype(int)\n","  \n","    accuracy = accuracy_score(gt,pred)\n","    precision, recall, f_score, support = prf(gt, pred, average='macro')\n","    print(\"================== Validation ==================\")\n","    print(\"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(accuracy, precision, recall, f_score))\n","    \n","    return accuracy, precision, recall, f_score\n","\n","\n","  def test(self):\n","    print(\"======================TEST MODE======================\")\n","    self.dagmm.eval()\n","\n","    self.data_loader.dataset.mode=\"train\"\n","    N = 0\n","    mu_sum = 0\n","    cov_sum = 0\n","    gamma_sum = 0\n","\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n","      \n","      batch_gamma_sum = torch.sum(gamma, dim=0)\n","      \n","      gamma_sum += batch_gamma_sum\n","      mu_sum += mu * batch_gamma_sum.unsqueeze(-1) # keep sums of the numerator only\n","      cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1) # keep sums of the numerator only\n","      \n","      N += input_data.size(0)\n","      \n","    train_phi = gamma_sum / N\n","    train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n","    train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n","\n","    train_energy = []\n","    train_labels = []\n","    train_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      # sample_energy, cov_diag = self.dagmm.compute_energy(z, size_average=False)\n","\n","      train_energy.append(sample_energy.data.cpu().numpy())\n","      train_z.append(z.data.cpu().numpy())\n","      train_labels.append(labels.numpy())\n","\n","    train_energy = np.concatenate(train_energy,axis=0)\n","    train_z = np.concatenate(train_z,axis=0)\n","    train_labels = np.concatenate(train_labels,axis=0)\n","    \n","    ###### use validation data\n","    self.data_loader.dataset.mode=\"valid\"\n","    valid_energy = []\n","    valid_labels = []\n","    valid_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      valid_energy.append(sample_energy.data.cpu().numpy())\n","      valid_z.append(z.data.cpu().numpy())\n","      valid_labels.append(labels.numpy())\n","\n","    valid_energy = np.concatenate(valid_energy,axis=0)\n","    valid_z = np.concatenate(valid_z,axis=0)\n","    valid_labels = np.concatenate(valid_labels,axis=0)\n","\n","    ###### use test data\n","    self.data_loader.dataset.mode=\"test\"\n","    test_energy = []\n","    test_labels = []\n","    test_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      test_energy.append(sample_energy.data.cpu().numpy())\n","      test_z.append(z.data.cpu().numpy())\n","      test_labels.append(labels.numpy())\n","\n","    test_energy = np.concatenate(test_energy,axis=0)\n","    test_z = np.concatenate(test_z,axis=0)\n","    test_labels = np.concatenate(test_labels,axis=0)\n","\n","    combined_energy = np.concatenate([train_energy, valid_energy, test_energy], axis=0)\n","    # combined_labels = np.concatenate([train_labels, valid_labels, test_labels], axis=0)\n","\n","    thresh = np.percentile(combined_energy, 100 - THR)\n","    print(\"Threshold :\", thresh)\n","\n","    pred = (test_energy > thresh).astype(int)\n","    gt = test_labels.astype(int)\n","    from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n","\n","    accuracy = accuracy_score(gt,pred)\n","    precision, recall, f_score, support = prf(gt, pred, average='macro')\n","    print(\"===========================================\")\n","    print(\"Test Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(accuracy, precision, recall, f_score))\n","    # return accuracy, precision, recall, f_score\n","    return pred"],"metadata":{"id":"0hkmHEtj8Pce","executionInfo":{"status":"ok","timestamp":1659665356684,"user_tz":-540,"elapsed":1081,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["def str2bool(v):\n","  return v.lower() in ('true')\n","\n","def main(config):\n","  # For fast training\n","  cudnn.benchmark = True\n","\n","  # Create directories if not exist\n","  mkdir(config.log_path)\n","  mkdir(config.model_save_path)\n","\n","  data_loader = get_loader(config.data_path, batch_size=config.batch_size, mode=config.mode)\n","  \n","  # Solver\n","  solver = Solver(data_loader, vars(config))\n","\n","  if config.mode == 'train':\n","    solver.train()\n","  elif config.mode == 'test':\n","    solver.test()\n","\n","  return solver\n","    \n","if __name__ == '__main__':\n","  wandb.init()\n","  import easydict\n","  args = easydict.EasyDict({\n","    \"num_epochs\" : 30,\n","    \"batch_size\" : 16384,\n","    \"gmm_k\" : 100, #4\n","    \"lambda_energy\" : 0.1,\n","    \"lambda_cov_diag\" : 0.005,\n","    # \"pretrained_model\" : '',\n","    \"pretrained_model\" : None,\n","    \"mode\" : 'train',\n","    # \"mode\" : \"test\",\n","    \"data_path\" : \"./kdd_cup.npz\",   \n","    \"use_tensorboard\" : False,\n","    \"log_path\" : './logs',\n","    \"model_save_path\" : './models',\n","    \"log_step\" : 194//4,\n","    \"sample_step\" : 194,\n","    \"model_save_step\" : 194,\n","    \"lr\" : 1e-6, #2e-5\n","    \"wd\" : None\n","  })\n","  config = args\n","\n","  print('------------ Options -------------')\n","  for k, v in sorted(args.items()):\n","    print('%s: %s' % (str(k), str(v)))\n","  print('-------------- End ----------------')\n","\n","  solver = main(config)\n","  pred = solver.test()"],"metadata":{"id":"O0vLb9Gp7pjL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = solver.test()"],"metadata":{"id":"0veCciFOz1gs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sum(pred)"],"metadata":{"id":"CyDzaKuOUfEC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659664812589,"user_tz":-540,"elapsed":376,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"4f18e5cd-c0f5-4b0a-c8bb-f23f0d33170e"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["141"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["## For prediction"],"metadata":{"id":"Fkq6EvUuUf38"}},{"cell_type":"code","source":["submit = pd.read_csv('./sample_submission.csv')\n","submit['Class'] = pred\n","submit.to_csv('./submit_220805(2).csv', index=False)"],"metadata":{"id":"2Al_KaztYQRh","executionInfo":{"status":"ok","timestamp":1659664820113,"user_tz":-540,"elapsed":881,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"J1ajB-lPYg8y"},"execution_count":null,"outputs":[]}]}