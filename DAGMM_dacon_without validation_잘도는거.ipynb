{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DAGMM_dacon_without validation_.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOGEOKVhrKi8LrkBP9dXqM/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"ba07515c85ae49c28d165422dbaacef0":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_caabcfdb8dae4867b5a6232f59fbfddf","IPY_MODEL_048ea883c946479c9408522ac4acf3b8"],"layout":"IPY_MODEL_df9e693f9fbe4fcfa46ceaf3a7454ebf"}},"caabcfdb8dae4867b5a6232f59fbfddf":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cef849bcc78a4a559dd697b3bdd3551b","placeholder":"​","style":"IPY_MODEL_f6278e1afcb440c49a624f7e960cb625","value":"0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\r"}},"048ea883c946479c9408522ac4acf3b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_86d041620af642ccbe13985a3c799bab","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d2e8c4c229bb45b3a594d9cbc3ff9346","value":1}},"df9e693f9fbe4fcfa46ceaf3a7454ebf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cef849bcc78a4a559dd697b3bdd3551b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6278e1afcb440c49a624f7e960cb625":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86d041620af642ccbe13985a3c799bab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2e8c4c229bb45b3a594d9cbc3ff9346":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ik9sLgGJ7G59","executionInfo":{"status":"ok","timestamp":1659199860214,"user_tz":-540,"elapsed":21683,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"c1bc8223-af51-4b00-bd8a-bd389498a784"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/MyDrive/IITP/sohyun/creditcard_prediction/data"]},{"cell_type":"code","source":["!pip install wandb -qqq\n","import wandb\n","wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"id":"P3CNrkhz2jjy","executionInfo":{"status":"ok","timestamp":1659199880578,"user_tz":-540,"elapsed":18075,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"4418a351-3377-47ae-9f53-e254a00bef57"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 1.8 MB 15.3 MB/s \n","\u001b[K     |████████████████████████████████| 156 kB 66.4 MB/s \n","\u001b[K     |████████████████████████████████| 181 kB 39.1 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import torch\n","import os\n","import random\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from PIL import Image\n","import h5py\n","import numpy as np\n","import collections\n","import numbers\n","import math\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import numpy as np\n","import os\n","import argparse\n","from torch.backends import cudnn\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","np.random.seed(0)\n","cudnn.benchmark = False\n","cudnn.deterministic = True\n","random.seed(0)"],"metadata":{"id":"61FVN9sCI2gt","executionInfo":{"status":"ok","timestamp":1659199890846,"user_tz":-540,"elapsed":5740,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv('./train.csv')\n","train_df = train_df.drop(columns=['ID'])\n","val_df = pd.read_csv('./val.csv')\n","val_df = val_df.drop(columns=['ID'])\n","test_df = pd.read_csv('./test.csv')\n","test_df = test_df.drop(columns=['ID'])\n","\n","#-------------------#\n","#---# Normalize #---#\n","#-------------------#\n","# case 1 - standardscaler\n","scaler_n = StandardScaler()\n","scaler_n.fit(train_df)\n","\n","val_x = val_df.drop(columns=['Class'])\n","train_x_scaleN = pd.DataFrame(scaler_n.transform(train_df), columns = train_df.columns) # 확인 : train_x_scaleN.mean(), train_x_scaleN.var()\n","val_x_scaleN = pd.DataFrame(scaler_n.transform(val_x), columns = val_x.columns)\n","test_x_scaleN = pd.DataFrame(scaler_n.transform(test_df), columns = test_df.columns)\n","\n","train_df = train_x_scaleN\n","val_df = pd.concat([val_x_scaleN, pd.DataFrame(val_df['Class'])], axis=1)\n","test_df = test_x_scaleN"],"metadata":{"id":"hsvD9j7WUHEz","executionInfo":{"status":"ok","timestamp":1659200372102,"user_tz":-540,"elapsed":5703,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def to_var(x, volatile=False):\n","  if torch.cuda.is_available():\n","      x = x.cuda()\n","  return Variable(x, volatile=volatile)\n","\n","def mkdir(directory):\n","  if not os.path.exists(directory):\n","      os.makedirs(directory)\n","\n","class KDD99Loader(object):\n","  def __init__(self, data_path, mode=\"train\"):\n","    self.mode=mode\n","    # data = np.load(data_path)\n","\n","    # labels = data[\"kdd\"][:,-1]\n","    # features = data[\"kdd\"][:,:-1]\n","    # N, D = features.shape\n","    \n","    # normal_data = features[labels==1]\n","    # normal_labels = labels[labels==1]\n","\n","    # N_normal = normal_data.shape[0]\n","\n","    # attack_data = features[labels==0]\n","    # attack_labels = labels[labels==0]\n","\n","    # N_attack = attack_data.shape[0]\n","\n","    # randIdx = np.arange(N_attack)\n","    # np.random.shuffle(randIdx)\n","    # N_train = N_attack // 2\n","\n","    # self.train = attack_data[randIdx[:N_train]]\n","    # self.train_labels = attack_labels[randIdx[:N_train]]\n","\n","    # self.test = attack_data[randIdx[N_train:]]\n","    # self.test_labels = attack_labels[randIdx[N_train:]]\n","\n","    # self.test = np.concatenate((self.test, normal_data),axis=0)\n","    # self.test_labels = np.concatenate((self.test_labels, normal_labels),axis=0)\n","\n","    self.train = train_df.values\n","    self.train_labels = np.tile(0, len(train_df))\n","\n","    self.test = val_df.drop(columns = ['Class']).values\n","    self.test_labels = val_df['Class'].values\n","\n","    # self.test = test_df.values\n","    # self.test_labels = np.tile(0, len(test_df))\n","\n","  def __len__(self):\n","    \"\"\"\n","    Number of images in the object dataset.\n","    \"\"\"\n","    if self.mode == \"train\":\n","      return self.train.shape[0]\n","    else:\n","      return self.test.shape[0]\n","\n","  def __getitem__(self, index):\n","    if self.mode == \"train\":\n","      return np.float32(self.train[index]), np.float32(self.train_labels[index])\n","    else:\n","      return np.float32(self.test[index]), np.float32(self.test_labels[index])\n","      \n","\n","def get_loader(data_path, batch_size, mode='train'):\n","  \"\"\"Build and return data loader.\"\"\"\n","  dataset = KDD99Loader(data_path, mode)\n","\n","  shuffle = False\n","  if mode == 'train':\n","    shuffle = True\n","\n","  data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n","  return data_loader"],"metadata":{"id":"uSLkzBK08gZE","executionInfo":{"status":"ok","timestamp":1659200379669,"user_tz":-540,"elapsed":356,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import torchvision\n","from torch.autograd import Variable\n","import itertools\n","\n","class Cholesky(torch.autograd.Function):\n","  def forward(ctx, a):\n","    l = torch.cholesky(a, False)\n","    ctx.save_for_backward(l)\n","    return l\n","  def backward(ctx, grad_output):\n","    l, = ctx.saved_variables\n","    linv = l.inverse()\n","    inner = torch.tril(torch.mm(l.t(), grad_output)) * torch.tril(\n","        1.0 - Variable(l.data.new(l.size(1)).fill_(0.5).diag()))\n","    s = torch.mm(linv.t(), torch.mm(inner, linv))\n","    return s\n","  \n","class DaGMM(nn.Module):\n","  \"\"\"Residual Block.\"\"\"\n","  def __init__(self, n_gmm = 4, latent_dim=12):\n","    super(DaGMM, self).__init__()\n","\n","    layers = []\n","    # layers += [nn.Linear(118,60)]\n","    # layers += [nn.Tanh()]        \n","    # layers += [nn.Linear(60,30)]\n","    # layers += [nn.Tanh()]        \n","    layers += [nn.Linear(30,25)]\n","    layers += [nn.Tanh()]         \n","    layers += [nn.Linear(25,20)]\n","    layers += [nn.Tanh()]         \n","    layers += [nn.Linear(20,10)]\n","    # layers += [nn.Tanh()]         \n","    # layers += [nn.Linear(15,10)]\n","\n","    self.encoder = nn.Sequential(*layers)\n","\n","    layers = []\n","    # layers += [nn.Linear(10,15)]\n","    # layers += [nn.Tanh()]        \n","    layers += [nn.Linear(10,20)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Linear(20,25)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Linear(25,30)]\n","    # layers += [nn.Tanh()]        \n","    # layers += [nn.Linear(60,118)]\n","\n","    self.decoder = nn.Sequential(*layers)\n","\n","    layers = []\n","    layers += [nn.Linear(latent_dim,10)]\n","    layers += [nn.Tanh()]        \n","    layers += [nn.Dropout(p=0.5)]        \n","    layers += [nn.Linear(10,n_gmm)]\n","    layers += [nn.Softmax(dim=1)]\n","\n","\n","    self.estimation = nn.Sequential(*layers)\n","\n","    self.register_buffer(\"phi\", torch.zeros(n_gmm))\n","    self.register_buffer(\"mu\", torch.zeros(n_gmm,latent_dim))\n","    self.register_buffer(\"cov\", torch.zeros(n_gmm,latent_dim,latent_dim))\n","\n","  def relative_euclidean_distance(self, a, b):\n","    return (a-b).norm(2, dim=1) / a.norm(2, dim=1)\n","\n","  def forward(self, x):\n","    enc = self.encoder(x)\n","    dec = self.decoder(enc)\n","    rec_cosine = F.cosine_similarity(x, dec, dim=1)\n","    rec_euclidean = self.relative_euclidean_distance(x, dec)\n","    z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n","    gamma = self.estimation(z)\n","    return enc, dec, z, gamma\n","\n","  def compute_gmm_params(self, z, gamma):\n","    N = gamma.size(0)\n","    # K\n","    sum_gamma = torch.sum(gamma, dim=0)\n","\n","    # K\n","    phi = (sum_gamma / N)\n","    self.phi = phi.data\n","\n","    # K x D\n","    mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n","    self.mu = mu.data\n","    # z = N x D\n","    # mu = K x D\n","    # gamma N x K\n","\n","    # z_mu = N x K x D\n","    z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n","\n","    # z_mu_outer = N x K x D x D\n","    z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n","\n","    # K x D x D\n","    cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim = 0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n","    self.cov = cov.data\n","\n","    return phi, mu, cov\n","      \n","  def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n","    if phi is None: phi = to_var(self.phi)\n","    if mu is None: mu = to_var(self.mu)\n","    if cov is None: cov = to_var(self.cov)\n","\n","    k, D, _ = cov.size()\n","\n","    z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n","\n","    cov_inverse = []\n","    det_cov = []\n","    cov_diag = 0\n","    eps = 1e-12\n","    for i in range(k):\n","      # K x D x D\n","      cov_k = cov[i] + to_var(torch.eye(D)*eps)\n","      cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n","\n","      #det_cov.append(np.linalg.det(cov_k.data.cpu().numpy()* (2*np.pi)))\n","      det_cov.append((Cholesky.apply(cov_k.cpu() * (2*np.pi)).diag().prod()).unsqueeze(0))\n","      cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n","\n","    # K x D x D\n","    cov_inverse = torch.cat(cov_inverse, dim=0)\n","    # K\n","    det_cov = torch.cat(det_cov).cuda()\n","    #det_cov = to_var(torch.from_numpy(np.float32(np.array(det_cov))))\n","\n","    # N x K\n","    exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n","    # for stability (logsumexp)\n","    max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n","\n","    exp_term = torch.exp(exp_term_tmp - max_val)\n","\n","    # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (det_cov).unsqueeze(0), dim = 1) + eps)\n","    sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt(det_cov)).unsqueeze(0), dim = 1) + eps)\n","    # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt((2*np.pi)**D * det_cov)).unsqueeze(0), dim = 1) + eps)\n","\n","\n","    if size_average:\n","      sample_energy = torch.mean(sample_energy)\n","\n","    return sample_energy, cov_diag\n","\n","\n","  def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n","    recon_error = torch.mean((x - x_hat) ** 2)\n","    phi, mu, cov = self.compute_gmm_params(z, gamma)\n","    sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n","    loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n","    return loss, sample_energy, recon_error, cov_diag"],"metadata":{"id":"MplRhuqf8CBe","executionInfo":{"status":"ok","timestamp":1659200383147,"user_tz":-540,"elapsed":539,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import os\n","import time\n","import datetime\n","from torch.autograd import grad\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","import IPython\n","from tqdm import tqdm\n","\n","class Solver(object):\n","  DEFAULTS = {}   \n","  def __init__(self, data_loader, config):\n","    # Data loader\n","    self.__dict__.update(Solver.DEFAULTS, **config)\n","    self.data_loader = data_loader\n","\n","    # Build tensorboard if use\n","    self.build_model()\n","    # if self.use_tensorboard:\n","    #     self.build_tensorboard()\n","\n","    # Start with trained model\n","    if self.pretrained_model:\n","      self.load_pretrained_model()\n","\n","  def build_model(self):\n","    # Define model\n","    self.dagmm = DaGMM(self.gmm_k)\n","\n","    # Optimizers\n","    self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr)\n","\n","    # Print networks\n","    self.print_network(self.dagmm, 'DaGMM')\n","\n","    if torch.cuda.is_available():\n","      self.dagmm.cuda()\n","\n","  def print_network(self, model, name):\n","    num_params = 0\n","    for p in model.parameters():\n","      num_params += p.numel()\n","    print(name)\n","    print(model)\n","    print(\"The number of parameters: {}\".format(num_params))\n","\n","  def load_pretrained_model(self):\n","    self.dagmm.load_state_dict(torch.load(os.path.join(\n","      self.model_save_path, '{}_dagmm.pth'.format(self.pretrained_model))))\n","\n","    # print(\"phi\", self.dagmm.phi,\"mu\",self.dagmm.mu, \"cov\",self.dagmm.cov)\n","\n","    print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n","\n","  # def build_tensorboard(self):\n","  #     from logger import Logger\n","  #     self.logger = Logger(self.log_path)\n","\n","  def reset_grad(self):\n","    self.dagmm.zero_grad()\n","\n","  def to_var(self, x, volatile=False):\n","    if torch.cuda.is_available():\n","        x = x.cuda()\n","    return Variable(x, volatile=volatile)\n","\n","  def train(self):\n","      iters_per_epoch = len(self.data_loader)\n","\n","      # Start with trained model if exists\n","      if self.pretrained_model:\n","          start = int(self.pretrained_model.split('_')[0])\n","      else:\n","          start = 0\n","\n","      # Start training\n","      iter_ctr = 0\n","      start_time = time.time()\n","    \n","      self.ap_global_train = np.array([0,0,0])\n","      for e in range(start, self.num_epochs):\n","        sum_total_loss = 0; sum_sample_energy = 0; sum_recon_error = 0; sum_cov_diag = 0 \n","        for i, (input_data, labels) in enumerate(tqdm(self.data_loader)):\n","          iter_ctr += 1\n","          start = time.time()\n","\n","          input_data = self.to_var(input_data)\n","\n","          total_loss,sample_energy, recon_error, cov_diag = self.dagmm_step(input_data)\n","          # Logging\n","          loss = {}\n","          loss['total_loss'] = total_loss.data.item()\n","          loss['sample_energy'] = sample_energy.item()\n","          loss['recon_error'] = recon_error.item()\n","          loss['cov_diag'] = cov_diag.item()\n","\n","          sum_total_loss += total_loss.data.item()\n","          sum_sample_energy += sample_energy.item()\n","          sum_recon_error += recon_error.item()\n","          sum_cov_diag += cov_diag.item()\n","\n","\n","          # Print out log info\n","          # if (i+1) % self.log_step == 0:\n","          #     elapsed = time.time() - start_time\n","          #     total_time = ((self.num_epochs*iters_per_epoch)-(e*iters_per_epoch+i)) * elapsed/(e*iters_per_epoch+i+1)\n","          #     epoch_time = (iters_per_epoch-i)* elapsed/(e*iters_per_epoch+i+1)\n","              \n","          #     epoch_time = str(datetime.timedelta(seconds=epoch_time))\n","          #     total_time = str(datetime.timedelta(seconds=total_time))\n","          #     elapsed = str(datetime.timedelta(seconds=elapsed))\n","\n","          #     lr_tmp = []\n","          #     for param_group in self.optimizer.param_groups:\n","          #         lr_tmp.append(param_group['lr'])\n","          #     tmplr = np.squeeze(np.array(lr_tmp))\n","\n","          #     log = \"Elapsed {}/{} -- {} , Epoch [{}/{}], Iter [{}/{}], lr {}\".format(\n","          #         elapsed,epoch_time,total_time, e+1, self.num_epochs, i+1, iters_per_epoch, tmplr)\n","\n","          #     for tag, value in loss.items():\n","          #         log += \", {}: {:.4f}\".format(tag, value)\n","\n","          #     IPython.display.clear_output()\n","          #     print(log)\n","\n","              # if self.use_tensorboard:\n","              #     for tag, value in loss.items():\n","              #         self.logger.scalar_summary(tag, value, e * iters_per_epoch + i + 1)\n","              # else:\n","              #     plt_ctr = 1\n","              #     if not hasattr(self,\"loss_logs\"):\n","              #         self.loss_logs = {}\n","              #         for loss_key in loss:\n","              #             self.loss_logs[loss_key] = [loss[loss_key]]\n","              #             plt.subplot(2,2,plt_ctr)\n","              #             plt.plot(np.array(self.loss_logs[loss_key]), label=loss_key)\n","              #             plt.legend()\n","              #             plt_ctr += 1\n","              #     else:\n","              #         for loss_key in loss:\n","              #             self.loss_logs[loss_key].append(loss[loss_key])\n","              #             plt.subplot(2,2,plt_ctr)\n","              #             plt.plot(np.array(self.loss_logs[loss_key]), label=loss_key)\n","              #             plt.legend()\n","              #             plt_ctr += 1\n","\n","              #     plt.show()\n","\n","          # print(\"phi\", self.dagmm.phi,\"mu\",self.dagmm.mu, \"cov\",self.dagmm.cov)\n","          # Save model checkpoints\n","      \n","          if (i+1) % self.model_save_step == 0:\n","            torch.save(self.dagmm.state_dict(),\n","                os.path.join(self.model_save_path, '{}_{}_dagmm.pth'.format(e+1, i+1)))\n","                \n","          # print(\"total_e\", sum_total_loss)\n","          # print(\"\", sum_sample_energy)\n","          # print(\"re\", sum_recon_error)\n","          # print(\"cov\", sum_cov_diag)\n","          # print(\"--\")\n","        wandb.log({\n","            \"total loss\": sum_total_loss,\n","            \"sample energy\": sum_sample_energy,\n","            \"recon error\": sum_recon_error,\n","            \"cov\": sum_cov_diag\n","        })\n","\n","  def dagmm_step(self, input_data):\n","    self.dagmm.train()\n","    enc, dec, z, gamma = self.dagmm(input_data)\n","\n","    total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma, self.lambda_energy, self.lambda_cov_diag)\n","\n","    self.reset_grad()\n","    total_loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(self.dagmm.parameters(), 5)\n","    self.optimizer.step()\n","\n","    return total_loss,sample_energy, recon_error, cov_diag\n","\n","  def test(self):\n","    print(\"======================TEST MODE======================\")\n","    self.dagmm.eval()\n","    self.data_loader.dataset.mode=\"train\"\n","\n","    N = 0\n","    mu_sum = 0\n","    cov_sum = 0\n","    gamma_sum = 0\n","\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n","      \n","      batch_gamma_sum = torch.sum(gamma, dim=0)\n","      \n","      gamma_sum += batch_gamma_sum\n","      mu_sum += mu * batch_gamma_sum.unsqueeze(-1) # keep sums of the numerator only\n","      cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1) # keep sums of the numerator only\n","      \n","      N += input_data.size(0)\n","      \n","    train_phi = gamma_sum / N\n","    train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n","    train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n","\n","    # print(\"N:\",N)\n","    # print(\"phi :\\n\",train_phi)\n","    # print(\"mu :\\n\",train_mu)\n","    # print(\"cov :\\n\",train_cov)\n","\n","    train_energy = []\n","    train_labels = []\n","    train_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      # sample_energy, cov_diag = self.dagmm.compute_energy(z, size_average=False)\n","\n","      train_energy.append(sample_energy.data.cpu().numpy())\n","      train_z.append(z.data.cpu().numpy())\n","      train_labels.append(labels.numpy())\n","\n","\n","    train_energy = np.concatenate(train_energy,axis=0)\n","    train_z = np.concatenate(train_z,axis=0)\n","    train_labels = np.concatenate(train_labels,axis=0)\n","\n","\n","    self.data_loader.dataset.mode=\"test\"\n","    test_energy = []\n","    test_labels = []\n","    test_z = []\n","    for it, (input_data, labels) in enumerate(self.data_loader):\n","      input_data = self.to_var(input_data)\n","      enc, dec, z, gamma = self.dagmm(input_data)\n","      sample_energy, cov_diag = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov, size_average=False)\n","      test_energy.append(sample_energy.data.cpu().numpy())\n","      test_z.append(z.data.cpu().numpy())\n","      test_labels.append(labels.numpy())\n","\n","    test_energy = np.concatenate(test_energy,axis=0)\n","    test_z = np.concatenate(test_z,axis=0)\n","    test_labels = np.concatenate(test_labels,axis=0)\n","\n","    combined_energy = np.concatenate([train_energy, test_energy], axis=0)\n","    combined_labels = np.concatenate([train_labels, test_labels], axis=0)\n","\n","    thresh = np.percentile(combined_energy, 100 - 0.1)\n","    print(\"Threshold :\", thresh)\n","\n","    pred = (test_energy > thresh).astype(int)\n","    gt = test_labels.astype(int)\n","    print(\"================================sum\", sum(gt))\n","    from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n","\n","    accuracy = accuracy_score(gt,pred)\n","    precision, recall, f_score, support = prf(gt, pred, average='macro')\n","    \n","    print(\"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f}\".format(accuracy, precision, recall, f_score))\n","    \n","    return accuracy, precision, recall, f_score"],"metadata":{"id":"0hkmHEtj8Pce","executionInfo":{"status":"ok","timestamp":1659200385894,"user_tz":-540,"elapsed":902,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def str2bool(v):\n","  return v.lower() in ('true')\n","\n","def main(config):\n","  # For fast training\n","  cudnn.benchmark = True\n","\n","  # Create directories if not exist\n","  mkdir(config.log_path)\n","  mkdir(config.model_save_path)\n","\n","  data_loader = get_loader(config.data_path, batch_size=config.batch_size, mode=config.mode)\n","  \n","  # Solver\n","  solver = Solver(data_loader, vars(config))\n","\n","  if config.mode == 'train':\n","    solver.train()\n","  elif config.mode == 'test':\n","    solver.test()\n","\n","  return solver\n","    \n","if __name__ == '__main__':\n","  wandb.init()\n","  import easydict\n","  args = easydict.EasyDict({\n","      \"num_epochs\" : 40,\n","      \"batch_size\" : 4096,\n","      \"gmm_k\" : 3,\n","      \"lambda_energy\" : 0.1,\n","      \"lambda_cov_diag\" : 0.005,\n","      # \"pretrained_model\" : '',\n","      \"pretrained_model\" : None,\n","      \"mode\" : 'train',\n","      # \"mode\" : \"test\",\n","      \"data_path\" : \"./kdd_cup.npz\",   \n","      \"use_tensorboard\" : False,\n","      \"log_path\" : './logs',\n","      \"model_save_path\" : './models',\n","      \"log_step\" : 194//4,\n","      \"sample_step\" : 194,\n","      \"model_save_step\" : 194,\n","      \"lr\" : 1e-7, #2e-5\n","      \"wd\" : None\n","  })\n","  config = args\n","\n","  print('------------ Options -------------')\n","  for k, v in sorted(args.items()):\n","    print('%s: %s' % (str(k), str(v)))\n","  print('-------------- End ----------------')\n","\n","  solver = main(config)\n","  solver.test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ba07515c85ae49c28d165422dbaacef0","caabcfdb8dae4867b5a6232f59fbfddf","048ea883c946479c9408522ac4acf3b8","df9e693f9fbe4fcfa46ceaf3a7454ebf","cef849bcc78a4a559dd697b3bdd3551b","f6278e1afcb440c49a624f7e960cb625","86d041620af642ccbe13985a3c799bab","d2e8c4c229bb45b3a594d9cbc3ff9346"]},"id":"O0vLb9Gp7pjL","executionInfo":{"status":"ok","timestamp":1659200449751,"user_tz":-540,"elapsed":61569,"user":{"displayName":"아이덴티파이ai","userId":"09195867153538576050"}},"outputId":"6bbd26a3-7c21-4f4d-9f5b-44c000202295"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:17vskxhh) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba07515c85ae49c28d165422dbaacef0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">firm-frost-228</strong>: <a href=\"https://wandb.ai/sohyun/uncategorized/runs/17vskxhh\" target=\"_blank\">https://wandb.ai/sohyun/uncategorized/runs/17vskxhh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20220730_165908-17vskxhh/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:17vskxhh). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.12.21"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/IITP/sohyun/creditcard_prediction/data/wandb/run-20220730_165947-4m7xj48e</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/sohyun/uncategorized/runs/4m7xj48e\" target=\"_blank\">blooming-sky-229</a></strong> to <a href=\"https://wandb.ai/sohyun/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["------------ Options -------------\n","batch_size: 4096\n","data_path: ./kdd_cup.npz\n","gmm_k: 3\n","lambda_cov_diag: 0.005\n","lambda_energy: 0.1\n","log_path: ./logs\n","log_step: 48\n","lr: 1e-07\n","mode: train\n","model_save_path: ./models\n","model_save_step: 194\n","num_epochs: 40\n","pretrained_model: None\n","sample_step: 194\n","use_tensorboard: False\n","wd: None\n","-------------- End ----------------\n","DaGMM\n","DaGMM(\n","  (encoder): Sequential(\n","    (0): Linear(in_features=30, out_features=25, bias=True)\n","    (1): Tanh()\n","    (2): Linear(in_features=25, out_features=20, bias=True)\n","    (3): Tanh()\n","    (4): Linear(in_features=20, out_features=10, bias=True)\n","  )\n","  (decoder): Sequential(\n","    (0): Linear(in_features=10, out_features=20, bias=True)\n","    (1): Tanh()\n","    (2): Linear(in_features=20, out_features=25, bias=True)\n","    (3): Tanh()\n","    (4): Linear(in_features=25, out_features=30, bias=True)\n","  )\n","  (estimation): Sequential(\n","    (0): Linear(in_features=12, out_features=10, bias=True)\n","    (1): Tanh()\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=10, out_features=3, bias=True)\n","    (4): Softmax(dim=1)\n","  )\n",")\n","The number of parameters: 3193\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/28 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n","L = torch.cholesky(A)\n","should be replaced with\n","L = torch.linalg.cholesky(A)\n","and\n","U = torch.cholesky(A, upper=True)\n","should be replaced with\n","U = torch.linalg.cholesky(A).mH().\n","This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1744.)\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n","  from ipykernel import kernelapp as app\n","100%|██████████| 28/28 [00:05<00:00,  5.34it/s]\n","100%|██████████| 28/28 [00:01<00:00, 25.14it/s]\n","100%|██████████| 28/28 [00:01<00:00, 26.05it/s]\n","100%|██████████| 28/28 [00:01<00:00, 27.54it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.95it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.80it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.39it/s]\n","100%|██████████| 28/28 [00:01<00:00, 27.52it/s]\n","100%|██████████| 28/28 [00:01<00:00, 25.04it/s]\n","100%|██████████| 28/28 [00:01<00:00, 25.36it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.61it/s]\n","100%|██████████| 28/28 [00:01<00:00, 27.58it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.69it/s]\n","100%|██████████| 28/28 [00:01<00:00, 25.34it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.65it/s]\n","100%|██████████| 28/28 [00:01<00:00, 27.26it/s]\n","100%|██████████| 28/28 [00:01<00:00, 25.38it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.70it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.93it/s]\n","100%|██████████| 28/28 [00:01<00:00, 23.94it/s]\n","100%|██████████| 28/28 [00:01<00:00, 27.72it/s]\n","100%|██████████| 28/28 [00:01<00:00, 17.00it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.64it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.82it/s]\n","100%|██████████| 28/28 [00:01<00:00, 27.05it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.21it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.32it/s]\n","100%|██████████| 28/28 [00:01<00:00, 26.77it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.02it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.04it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.42it/s]\n","100%|██████████| 28/28 [00:01<00:00, 27.35it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.36it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.77it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.95it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.88it/s]\n","100%|██████████| 28/28 [00:01<00:00, 24.88it/s]\n","100%|██████████| 28/28 [00:01<00:00, 27.23it/s]\n","100%|██████████| 28/28 [00:01<00:00, 25.14it/s]\n","100%|██████████| 28/28 [00:01<00:00, 25.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["======================TEST MODE======================\n","Threshold : 27.63100814819336\n","================================sum 30\n","Accuracy : 0.9990, Precision : 0.7590, Recall : 0.7331, F-score : 0.7454\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"AFJO8kET6dTU"},"execution_count":null,"outputs":[]}]}